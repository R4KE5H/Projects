{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e80879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\python.exe\n"
     ]
    }
   ],
   "source": [
    "'''Importing the dependencies and libraries'''\n",
    "import sys, dlib, random, os, time\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Analytics, ML and Visualization libraries\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as img_plt \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from utils.download import download_url_to_file\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4b6ad",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc73fa",
   "metadata": {},
   "source": [
    "1. Provide the required values for the variables (face_detector, img_path, data_fol, im_size) that are assinged!\n",
    "2. Provide the classes folder name in numeric values like (0,1,2,...)\n",
    "3. Provide the faces images in respected classes folder.\n",
    "Data Folder Structure:\n",
    "        --> data:\n",
    "               --> class 1:\n",
    "                       --> Image 1\n",
    "                       --> Image 2\n",
    "                       --> Image 3\n",
    "                           .\n",
    "                           .\n",
    "                           .\n",
    "               --> class 2:\n",
    "                       --> Image 1\n",
    "                       --> Image 2\n",
    "                       --> Image 3\n",
    "                           .\n",
    "                           .\n",
    "                           ....\n",
    "\n",
    "                   .\n",
    "                   .\n",
    "                   ....\n",
    "4. Provide face detector 'dlib' or None. Default cv2 cascade classifier is selected.\n",
    "5. Provide the  im_size depends of model input shape [224*224] is selected currently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adf5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigin \"dlib\" to run dib face detector\n",
    "# Assign \"None\" to run cv2 Cascade Classifier \n",
    "face_detector=\"dlib\" \n",
    "img_path= r\"path\\Claudia_Pechstein_0003.jpg\" # image path \n",
    "data_fol=r\"path\" # provide the data directory path\n",
    "im_size= 224#image size\n",
    "\n",
    "# Default 'cv2.CascadeClassifier' is selected \n",
    "if face_detector==\"dlib\":\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "else:    \n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0e9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select True if you want to view the image\n",
    "if False:\n",
    "    img = cv2.imread(img_path) \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    if face_detector==\"dlib\":\n",
    "        faces = detector(gray)\n",
    "        print(faces)\n",
    "        for face in faces:\n",
    "            x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2) \n",
    "    else:\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "        for (x,y,w,h) in faces: \n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2) \n",
    "            roi_gray = gray[y:y+h, x:x+w] \n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "                \n",
    "    cv2.imshow('img',img) \n",
    "    cv2.waitKey(10)\n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92676e",
   "metadata": {},
   "source": [
    "To prepare \"data.csv\" datafame with columns \"Image Path\", \"Face\", \"Bounding Box\", \"Aspect ratio\", \"Target\" for training purpose with pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bc55c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes=os.listdir(data_fol)\n",
    "rows_list=[]\n",
    "for i in tqdm(classes):\n",
    "    fol=data_fol+f\"\\{i}\"\n",
    "    imgs=os.listdir(fol)\n",
    "    for j in imgs:\n",
    "        img = cv2.imread(fol+f\"\\{j}\") \n",
    "        img=cv2.resize(img, (im_size, im_size), interpolation = cv2.INTER_LINEAR)\n",
    "        H, W, c = img.shape\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "        if face_detector==\"dlib\":\n",
    "            faces = detector(gray)\n",
    "            cls_n=None\n",
    "            if faces:\n",
    "                face=faces[0]\n",
    "                x, y, w, h = (face.left(), face.top(), face.width(), face.height());bb=abs(x),abs(y),abs(x+w),abs(y+h)\n",
    "            else:\n",
    "                bb= (0,0,W,H)\n",
    "                cls_n=len(classes)+1   \n",
    "            rows_list.append({'Image Path':fol+f\"\\{j}\", 'Face': 1 if faces else 0, 'Bounding Box':bb ,'Aspect ratio':(w,h),'Target':cls_n if cls_n else i})\n",
    "        else:\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "            cls_n=None\n",
    "            if type(faces)!=tuple and faces.any():\n",
    "                x,y,w,h=faces[0];bb=abs(x),abs(y),abs(x+w),abs(y+h)\n",
    "            else:\n",
    "                bb= (0,0,W,H)\n",
    "                cls_n=len(classes)+1\n",
    "            rows_list.append({'Image Path':fol+f\"\\{j}\", 'Face': 1 if type(faces)!=tuple and faces.any() else 0, 'Bounding Box':bb ,'Aspect ratio':(w,h),'Target':cls_n if cls_n else i})\n",
    "df = pd.DataFrame(rows_list) \n",
    "df.astype(object)\n",
    "display(df.head())   \n",
    "df.to_csv(\"data.csv\",index=False)\n",
    "print(\"================= 'data.csv' Generated =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dd65ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7606 entries, 0 to 7605\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Image Path    7606 non-null   object\n",
      " 1   Face          7606 non-null   int64 \n",
      " 2   Bounding Box  7606 non-null   object\n",
      " 3   Aspect ratio  7606 non-null   object\n",
      " 4   Target        7606 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 297.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # provide the details of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82542985",
   "metadata": {},
   "source": [
    "kmeans cluster and k nearest neighbor algorithim is used to find anchor and to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b442fec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clustering Algorithm\n",
    "def kmeans_anchors(boxes, k):\n",
    "    # Normalize boxes\n",
    "    boxes = np.array(boxes)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(boxes)\n",
    "    anchors = kmeans.cluster_centers_\n",
    "    return anchors\n",
    "\n",
    "face_df=df[df[\"Face\"]==1]\n",
    "print(face_df.shape)\n",
    "anchor_=kmeans_anchors(face_df[\"Aspect ratio\"].tolist(),1)\n",
    "anchor_=np.array(anchor_[0]).astype(int)\n",
    "face_df[\"Anchor\"]=[list(anchor_)]*face_df.shape[0]\n",
    "\n",
    "''' To find cluster with respective to classes'''\n",
    "# anchor_dic={}\n",
    "# for un_ in face_df[\"Target\"].unique().tolist():\n",
    "#     tempdf=face_df[face_df[\"Target\"]==un_]\n",
    "#     anchor_dic.update({un_:kmeans_anchors(tempdf[\"Aspect ratio\"].tolist(), 1)[0]})\n",
    "# print(anchor_dic)  \n",
    "\n",
    "display(face_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e7f638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchor_im={} #anchor image\n",
    "anchor_bb={} #anchor bounding box\n",
    "\n",
    "for un_ in tqdm(face_df[\"Target\"].unique().tolist()):\n",
    "    tempdf=face_df[face_df[\"Target\"]==un_]\n",
    "    points=[tuple(tempdf[\"Anchor\"].tolist()[0])]+tempdf[\"Aspect ratio\"].tolist()\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(points)\n",
    "    distances, indices = nbrs.kneighbors(points)\n",
    "    if indices.any():\n",
    "        anchor_im.update({un_:tempdf.iloc[indices[0][1]-1][\"Image Path\"]})\n",
    "        anchor_bb.update({un_:tempdf.iloc[indices[0][1]-1][\"Bounding Box\"]})\n",
    "\n",
    "def anchor_image(row):\n",
    "    global anchor_im\n",
    "    return anchor_im[row[\"Target\"]]\n",
    "\n",
    "def anchor_boundingbox(row):\n",
    "    global anchor_bb\n",
    "    return anchor_bb[row[\"Target\"]]    \n",
    "    \n",
    "    \n",
    "\n",
    "face_df['Anchor Image'] = face_df.apply(anchor_image, axis=1)\n",
    "face_df['Anchor bb'] = face_df.apply(anchor_boundingbox, axis=1)\n",
    "\n",
    "display(face_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4f057",
   "metadata": {},
   "source": [
    "To prepare final \"face_data.csv\" datafame with columns \"Image Path\", \"Face\", \"Bounding Box\", \"Aspect ratio\", \"Target\", \"Anchor\", \"Anchor Image\", \"Anchor bb\", \"Negative Image\", \"Negative bb\" for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1d6eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_ls=[] #negative image path\n",
    "neg_bb=[] #negative face bounding box\n",
    "\n",
    "for row in tqdm(face_df.iterrows()):\n",
    "    ex_id=row[1][\"Target\"]\n",
    "    tempdf=face_df[face_df[\"Target\"]!=ex_id]\n",
    "    rand_path=random.choice(tempdf[\"Image Path\"].tolist())\n",
    "    rand_bb=tempdf[tempdf[\"Image Path\"]==rand_path][\"Bounding Box\"].tolist()[0]\n",
    "    neg_ls.append(rand_path)\n",
    "    neg_bb.append(rand_bb)\n",
    "\n",
    "face_df[\"Negative Image\"]=neg_ls\n",
    "face_df[\"Negative bb\"]=neg_bb\n",
    "display(face_df.head())\n",
    "face_df.to_csv(\"face_data.csv\",index=False)\n",
    "print(\"++++++++++++++++++++++++ 'face_data.csv' is Generated ++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d22d18",
   "metadata": {},
   "source": [
    "Visualising and analysing the non detected face images by face detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c55592c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df[df[\"Face\"]==0]); len(df[df[\"Face\"]==0]) # Filtering the non-detected face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c3fc59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising  the non- detected face and anlysis\n",
    "for row in df[df[\"Face\"]==0].iterrows():\n",
    "    print(row[1][\"Image Path\"])\n",
    "    im = img_plt.imread(row[1][\"Image Path\"]) \n",
    "    plt.imshow(im) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6578380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of face detected by dlib face detector in provided dataset 99.35577175913753\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dlib face detector detected around 99% of face with provided dataset\n",
    "cv2 cascade classifier detected around 95% of face with provided dataset\n",
    "'''\n",
    "\n",
    "print(\"Percentage of face detected by dlib face detector in provided dataset\",(df[\"Face\"].sum()/df.shape[0])*100) # cv2 face cascade 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d7b4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ecb51",
   "metadata": {},
   "source": [
    "# FaceNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91838a7",
   "metadata": {},
   "source": [
    "FaceNet is the name of the facial recognition system that was proposed by Google Researchers in 2015 in the paper titled FaceNet: A Unified Embedding for Face Recognition and Clustering.\n",
    "\n",
    "Reference:\n",
    "\n",
    "    -> https://github.com/davidsandberg/facenet\n",
    "    -> https://github.com/timesler/facenet-pytorch/tree/master\n",
    "    -> https://medium.com/@mohitsaini_54300/train-facenet-with-triplet-loss-for-real-time-face-recognition-a39e2f4472c3\n",
    "    -> https://datahacker.rs/025-facenet-a-unified-embedding-for-face-recognition-and-clustering-in-pytorch/\n",
    "    -> https://www.geeksforgeeks.org/facenet-using-facial-recognition-system/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cf4f8",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "            Input images [anchor, positive, negative]\n",
    "            Batch Input [Tensor:torch.Size([1, 3, Image size, Image size])]\n",
    "                               ⬇️\n",
    "            FaceNet DNN network (InceptionResnetV1 model)\n",
    "                    (pretrained vggface weights)\n",
    "                               ⬇️\n",
    "                face embbedings [torch.Size([1, 512])]\n",
    "                               ⬇️\n",
    "                      Triplet loss function                      \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e70b8b",
   "metadata": {},
   "source": [
    "This is a Pytorch implementation of the face recognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17dbe4",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f23bcbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 10, 100, 101, 102, 103, 104, 105, 106, 107, 108, 901, 109, 11, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 12, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 13, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 14, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 15, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 16, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 17, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 18, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 19, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 2, 20, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 21, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 22, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 23, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 24, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 25, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 26, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 27, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 28, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 29, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 3, 30, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 31, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 32, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 33, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 34, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 35, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 36, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 37, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 38, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 39, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 4, 40, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 41, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 42, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 43, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 44, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 45, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 46, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 47, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 48, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 49, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 5, 50, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 51, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 52, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 53, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 54, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 55, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 56, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 57, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 58, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 59, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 6, 60, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 61, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 62, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 63, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 64, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 65, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 66, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 67, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 68, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 69, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 7, 70, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 71, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 72, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 73, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 74, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 75, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 76, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 77, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 78, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 79, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 8, 80, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 81, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 82, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 83, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 84, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 85, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 86, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 87, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 88, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 89, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 9, 90, 900, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "imageSize=224 #input image size\n",
    "\n",
    "'''\n",
    "Custom dataloader for Facenet model.\n",
    "Returns: anchor (Tensor:torch.Size([1, 3, 224, 224])), positive (Tensor:torch.Size([1, 3, 224, 224])), negative (Tensor:torch.Size([1, 3, 224, 224]))\n",
    "'''\n",
    "class classificationDataset(Dataset):\n",
    "    def __init__(self, csv_path, imageSize,transform=None):\n",
    "        self.df_data=pd.read_csv(csv_path)\n",
    "        self.df_data.loc[self.df_data['Target'] == 902, 'Target'] = 901\n",
    "        self.transform = transform\n",
    "        self.classes = self.df_data[\"Target\"].unique().tolist()\n",
    "        self.images = self.df_data[\"Image Path\"].tolist()\n",
    "        self.labels = self.df_data[\"Target\"].tolist()\n",
    "        self.face_detector = self.df_data[\"Bounding Box\"].tolist()\n",
    "        self.im_size=imageSize\n",
    "        print(self.classes)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image=cv2.resize(image, (self.im_size, self.im_size), interpolation = cv2.INTER_LINEAR)\n",
    "        face_detect=eval(self.face_detector[idx])\n",
    "        image=image[face_detect[1]:face_detect[3],face_detect[0]:face_detect[2]]\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "# Define transformations for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imageSize, imageSize)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Create the dataset and dataloader\n",
    "dataset = classificationDataset('./data.csv', imageSize, transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# Example of iterating through the dataloader\n",
    "for images, labels in dataloader:\n",
    "    print(images.size(), labels.size())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba689d54",
   "metadata": {},
   "source": [
    "# Model arhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a962bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_home():\n",
    "    torch_home = os.path.expanduser(\n",
    "        os.getenv(\n",
    "            'TORCH_HOME',\n",
    "            os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')\n",
    "        )\n",
    "    )\n",
    "    return torch_home\n",
    "\n",
    "def load_weights(mdl):\n",
    "    path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt' #'vggface2' weights\n",
    "    \n",
    "    model_dir = os.path.join(get_torch_home(), 'checkpoints')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    cached_file = os.path.join(model_dir, os.path.basename(path))\n",
    "    if not os.path.exists(cached_file):\n",
    "        download_url_to_file(path, cached_file)\n",
    "\n",
    "    state_dict = torch.load(cached_file)\n",
    "    mdl.load_state_dict(state_dict)\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_planes, out_planes,\n",
    "            kernel_size=kernel_size, stride=stride,\n",
    "            padding=padding, bias=False\n",
    "        ) # verify bias false\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_planes,\n",
    "            eps=0.001, # value found in tensorflow\n",
    "            momentum=0.1, # default pytorch value\n",
    "            affine=True\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block35(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block17(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 128, kernel_size=1, stride=1),\n",
    "            BasicConv2d(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(128, 128, kernel_size=(7,1), stride=1, padding=(3,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block8(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0, noReLU=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.noReLU = noReLU\n",
    "\n",
    "        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(1792, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
    "            BasicConv2d(192, 192, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\n",
    "        if not self.noReLU:\n",
    "            self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        if not self.noReLU:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_6a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_7a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InceptionResnetV1(nn.Module):\n",
    "    def __init__(self, pretrained=\"vggface2\", classify=True, num_classes=None, dropout_prob=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.classify = classify\n",
    "        self.num_classes = num_classes\n",
    "        tmp_classes = 8631  #'vggface2' output layer\n",
    "\n",
    "        # Define layers\n",
    "        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n",
    "        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n",
    "        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        self.repeat_1 = nn.Sequential(\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "        )\n",
    "        self.mixed_6a = Mixed_6a()\n",
    "        self.repeat_2 = nn.Sequential(\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "        )\n",
    "        self.mixed_7a = Mixed_7a()\n",
    "        self.repeat_3 = nn.Sequential(\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "        )\n",
    "        self.block8 = Block8(noReLU=True)\n",
    "        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.last_linear = nn.Linear(1792, 512, bias=False)\n",
    "        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.logits = nn.Linear(512, tmp_classes)\n",
    "            load_weights(self)\n",
    "\n",
    "        if self.classify and self.num_classes is not None:\n",
    "            self.logits = nn.Linear(512, self.num_classes)\n",
    "            \n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(device)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate embeddings or logits given a batch of input image tensors.\n",
    "\n",
    "        Arguments:\n",
    "            x {torch.tensor} -- Batch of image tensors representing faces.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor -- Batch of embedding vectors or multinomial logits.\n",
    "        \"\"\"\n",
    "        x = self.conv2d_1a(x)\n",
    "        x = self.conv2d_2a(x)\n",
    "        x = self.conv2d_2b(x)\n",
    "        x = self.maxpool_3a(x)\n",
    "        x = self.conv2d_3b(x)\n",
    "        x = self.conv2d_4a(x)\n",
    "        x = self.conv2d_4b(x)\n",
    "        x = self.repeat_1(x)\n",
    "        x = self.mixed_6a(x)\n",
    "        x = self.repeat_2(x)\n",
    "        x = self.mixed_7a(x)\n",
    "        x = self.repeat_3(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.avgpool_1a(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x.view(x.shape[0], -1))\n",
    "        x = self.last_bn(x)\n",
    "        if self.classify:\n",
    "            x = self.logits(x)\n",
    "        else:\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e8e1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving checkpoint for every epochs\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    \n",
    "# loading checkpoint \n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"loaded\")\n",
    "    return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f4844d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "# Training Epochs\n",
    "num_epochs = 100 #number of epochs\n",
    "\n",
    "checkpoint_fol = './model_classiffication/' # provide directory of model to be saved\n",
    "checkpoint_path=\"./model_classiffication/13_Loss_0.2219.pt\" # provide the path to load the trained model\n",
    "\n",
    "model = InceptionResnetV1(num_classes=902) #intialze the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) #adam optimizer\n",
    "start_epoch=0\n",
    "# If 'checkpoint_path' not None and path is provided. The model will be loaded and weights will be intialized with trained weights\n",
    "if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "    model, optimizer, start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe408d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionResnetV1(\n",
       "  (conv2d_1a): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2d_2a): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2d_2b): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (maxpool_3a): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2d_3b): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2d_4a): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2d_4b): BasicConv2d(\n",
       "    (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (repeat_1): Sequential(\n",
       "    (0): Block35(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block35(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block35(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block35(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Block35(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mixed_6a): Mixed_6a(\n",
       "    (branch0): BasicConv2d(\n",
       "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (branch1): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (branch2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (repeat_2): Sequential(\n",
       "    (0): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (8): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (9): Block17(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mixed_7a): Mixed_7a(\n",
       "    (branch0): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (branch1): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): BasicConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (branch3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (repeat_3): Sequential(\n",
       "    (0): Block8(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block8(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block8(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block8(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Block8(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (block8): Block8(\n",
       "    (branch0): BasicConv2d(\n",
       "      (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (branch1): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (avgpool_1a): AdaptiveAvgPool2d(output_size=1)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       "  (last_linear): Linear(in_features=1792, out_features=512, bias=False)\n",
       "  (last_bn): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (logits): Linear(in_features=512, out_features=902, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a32265a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520af7f52fdc48e2858305b92d79ca04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5477e199b527475f8d23cd832162336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Loss: 4.0750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0c5d1feb604f2a8708486515f39311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Loss: 2.6243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d6ba9f91354d73851e7f03dcc1b614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Loss: 2.0237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b3c35fcb5649e88caad277900cec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Loss: 1.5408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f138abc04014beeaffdbe32177be988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Loss: 0.9188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac654b3ffd0441c08bf7c62f5c3a0d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Loss: 1.1563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb7466cbf674a7c8be08fe3a45a73d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Loss: 0.5076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3daddd5002473095d7c2a0d8c47819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Loss: 0.1125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a141f246d4e142ac9f3748097303af7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Loss: 0.2984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b492358b2b4bdc984ab4aa874f157d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Loss: 0.2902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe5bb69f98a44a0a7592cf552a9f79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Loss: 0.2324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e48a0442294f0fb6d4e1caf48e10ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] Loss: 0.2607\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93644ff739c046808bc7be7d4ba19556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] Loss: 0.0268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3f22f3696441fd90884bbe6b558c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m running_loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targets) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(scores, targets)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 272\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool_3a(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d_3b(x)\n\u001b[1;32m--> 272\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_4a\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d_4b(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[0;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(start_epoch,num_epochs)):\n",
    "    running_loss= 0.0\n",
    "    for batch_idx, (data, targets) in tqdm(enumerate(dataloader)):\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or Adam step\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}')\n",
    "    file_=f'{epoch+1}_Loss_{running_loss/len(dataloader):.4f}'\n",
    "    save_checkpoint(model, optimizer, epoch, running_loss, checkpoint_fol+f\"{file_}.pt\")\n",
    "print(\"Finished Training\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854a602",
   "metadata": {},
   "source": [
    "Re-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7e3154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6042ef53f14e44d4b7920e840f6c05cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7694fd05efb240cd9bb0e1d9cab6ea86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] Loss: 0.1549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93865559bdf4acab90ba538348b07dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Gradient descent or Adam step\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(start_epoch,num_epochs)):\n",
    "    running_loss= 0.0\n",
    "    for batch_idx, (data, targets) in tqdm(enumerate(dataloader)):\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or Adam step\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}')\n",
    "    file_=f'{epoch+1}_Loss_{running_loss/len(dataloader):.4f}'\n",
    "    save_checkpoint(model, optimizer, epoch, running_loss, checkpoint_fol+f\"{file_}.pt\")\n",
    "print(\"Finished Training\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c6df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.0089\n"
     ]
    }
   ],
   "source": [
    "'''To save the checkpoint for trained weights'''\n",
    "# print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "# file_=f'{epoch+1}_Loss_{running_loss/len(dataloader):.4f}'\n",
    "# save_checkpoint(model, optimizer, epoch, running_loss, checkpoint_fol+f\"{file_}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83446a3f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43577e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_classification():\n",
    "    im_size=224\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((im_size, im_size)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor()]) \n",
    "    \n",
    "    def __init__(self,model, optimizer,checkpoint):\n",
    "        self.model=model\n",
    "        if checkpoint:self.model=self.load_checkpoint(self.model, optimizer, checkpoint)\n",
    "        self.model.eval()   \n",
    "        self.device = torch.device('cpu')\n",
    "\n",
    "    def load_checkpoint(self,model, optimizer, path): \n",
    "        checkpoint = torch.load(path,map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(\"Loaded\")\n",
    "        return model    \n",
    "\n",
    "    def preprocess(self,image):\n",
    "        im_size= Face_classification.im_size\n",
    "        image=cv2.resize(image, (im_size, im_size), interpolation = cv2.INTER_LINEAR)\n",
    "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image=Image.fromarray(image)\n",
    "        return image \n",
    "\n",
    "    \n",
    "    def detect(self,img):\n",
    "        detect_img=self.preprocess(img)\n",
    "        detect_img = Face_classification.transform(detect_img)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            face_classes = self.model(detect_img.unsqueeze(0))\n",
    "        return face_classes       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76774d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "img_path=r\"path\\13\\Jesse_Jackson_0002.jpg\"\n",
    "face_detector=\"dlib\"\n",
    "im_size=224 #image size\n",
    "check_point_path=\"./model_classiffication/13_Loss_0.2219.pt\"\n",
    "\n",
    "model = InceptionResnetV1(num_classes=902)\n",
    "opt=optim.Adam(model.parameters(), lr=0.001)\n",
    "fr=Face_classification(model,opt,check_point_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24406cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.14190053939819336\n",
      "Predition:  {0: 13}\n"
     ]
    }
   ],
   "source": [
    "if face_detector==\"dlib\":\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "else:    \n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "    \n",
    "if True:\n",
    "    \n",
    "    start_time=time.time()\n",
    "    img = cv2.imread(img_path) \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    crop_img=[]\n",
    "    if face_detector==\"dlib\":\n",
    "        faces = detector(gray)\n",
    "        for face in faces:\n",
    "            x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "            # cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2)\n",
    "            crop_img.append(img[y:y+h,x:x+w])\n",
    "    else:\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "        for (x,y,w,h) in faces: \n",
    "            # cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2) \n",
    "            roi_gray = gray[y:y+h, x:x+w] \n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "            crop_img.append(roi_color)\n",
    "\n",
    "    results={}\n",
    "    \n",
    "    if crop_img:\n",
    "        for idx,c_img in enumerate(crop_img):\n",
    "            result=fr.detect(c_img)\n",
    "            cls=int(torch.argmax(result,dim=1))\n",
    "            results.update({idx:902 if cls==901 else cls})\n",
    "            \n",
    "    img=cv2.resize(img, (420,640), interpolation = cv2.INTER_LINEAR)    \n",
    "    img = cv2.putText(img, f\"Detected class: {results.values() if results else 'Nil' }\", (5, 10), cv2.FONT_HERSHEY_SIMPLEX,  0.5, (255, 0, 0) , 1, cv2.LINE_AA)       \n",
    "    print(\"Time taken: \",time.time()-start_time) \n",
    "    print(\"Predition: \", results)\n",
    "    cv2.imshow('img',img) \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8cb2bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[362.    ,   0.6016,   1.    ],\n",
       "       [341.    ,   0.6022,   1.    ],\n",
       "       [307.    ,   0.6076,   1.    ],\n",
       "       [841.    ,   0.6085,   1.    ],\n",
       "       [452.    ,   0.6092,   1.    ],\n",
       "       [592.    ,   0.6112,   1.    ],\n",
       "       [832.    ,   0.6119,   1.    ],\n",
       "       [626.    ,   0.6159,   1.    ],\n",
       "       [474.    ,   0.6161,   1.    ],\n",
       "       [194.    ,   0.62  ,   1.    ],\n",
       "       [481.    ,   0.6216,   1.    ],\n",
       "       [760.    ,   0.6223,   1.    ],\n",
       "       [378.    ,   0.6268,   1.    ],\n",
       "       [601.    ,   0.6312,   1.    ],\n",
       "       [805.    ,   0.6315,   1.    ],\n",
       "       [723.    ,   0.632 ,   1.    ],\n",
       "       [479.    ,   0.6323,   1.    ],\n",
       "       [614.    ,   0.6328,   1.    ],\n",
       "       [375.    ,   0.635 ,   1.    ],\n",
       "       [874.    ,   0.6373,   1.    ],\n",
       "       [541.    ,   0.6384,   1.    ],\n",
       "       [300.    ,   0.6387,   1.    ],\n",
       "       [696.    ,   0.642 ,   1.    ],\n",
       "       [241.    ,   0.648 ,   1.    ],\n",
       "       [488.    ,   0.6487,   1.    ],\n",
       "       [900.    ,   0.649 ,   1.    ],\n",
       "       [284.    ,   0.6493,   1.    ],\n",
       "       [728.    ,   0.6501,   1.    ],\n",
       "       [775.    ,   0.6526,   1.    ],\n",
       "       [669.    ,   0.6605,   1.    ],\n",
       "       [275.    ,   0.6629,   1.    ],\n",
       "       [174.    ,   0.6634,   1.    ],\n",
       "       [ 75.    ,   0.6671,   1.    ],\n",
       "       [ 37.    ,   0.6693,   1.    ],\n",
       "       [232.    ,   0.6714,   1.    ],\n",
       "       [837.    ,   0.6732,   1.    ],\n",
       "       [884.    ,   0.6732,   1.    ],\n",
       "       [697.    ,   0.6822,   1.    ],\n",
       "       [485.    ,   0.6832,   1.    ],\n",
       "       [768.    ,   0.6917,   1.    ],\n",
       "       [848.    ,   0.6928,   1.    ],\n",
       "       [113.    ,   0.6935,   1.    ],\n",
       "       [477.    ,   0.6966,   1.    ],\n",
       "       [ 35.    ,   0.7004,   1.    ],\n",
       "       [888.    ,   0.7017,   1.    ],\n",
       "       [ 99.    ,   0.7038,   1.    ],\n",
       "       [857.    ,   0.7039,   1.    ],\n",
       "       [736.    ,   0.7053,   1.    ],\n",
       "       [731.    ,   0.7053,   1.    ],\n",
       "       [317.    ,   0.7088,   1.    ],\n",
       "       [649.    ,   0.7099,   1.    ],\n",
       "       [542.    ,   0.7099,   1.    ],\n",
       "       [834.    ,   0.7117,   1.    ],\n",
       "       [225.    ,   0.7129,   1.    ],\n",
       "       [719.    ,   0.7139,   1.    ],\n",
       "       [122.    ,   0.717 ,   1.    ],\n",
       "       [496.    ,   0.7193,   1.    ],\n",
       "       [172.    ,   0.7193,   1.    ],\n",
       "       [408.    ,   0.72  ,   1.    ],\n",
       "       [377.    ,   0.725 ,   1.    ],\n",
       "       [239.    ,   0.7279,   1.    ],\n",
       "       [179.    ,   0.7291,   1.    ],\n",
       "       [635.    ,   0.73  ,   1.    ],\n",
       "       [661.    ,   0.7362,   1.    ],\n",
       "       [169.    ,   0.7364,   1.    ],\n",
       "       [422.    ,   0.7367,   1.    ],\n",
       "       [430.    ,   0.7373,   1.    ],\n",
       "       [139.    ,   0.7404,   1.    ],\n",
       "       [121.    ,   0.7412,   1.    ],\n",
       "       [701.    ,   0.7421,   1.    ],\n",
       "       [574.    ,   0.7444,   1.    ],\n",
       "       [593.    ,   0.7474,   1.    ],\n",
       "       [630.    ,   0.7495,   1.    ],\n",
       "       [765.    ,   0.7507,   1.    ],\n",
       "       [816.    ,   0.7519,   1.    ],\n",
       "       [230.    ,   0.7528,   1.    ],\n",
       "       [112.    ,   0.7553,   1.    ],\n",
       "       [511.    ,   0.7554,   1.    ],\n",
       "       [780.    ,   0.7586,   1.    ],\n",
       "       [792.    ,   0.7614,   1.    ],\n",
       "       [526.    ,   0.7644,   1.    ],\n",
       "       [794.    ,   0.7662,   1.    ],\n",
       "       [566.    ,   0.7726,   1.    ],\n",
       "       [400.    ,   0.7744,   1.    ],\n",
       "       [152.    ,   0.7744,   1.    ],\n",
       "       [393.    ,   0.7776,   1.    ],\n",
       "       [435.    ,   0.7816,   1.    ],\n",
       "       [244.    ,   0.7817,   1.    ],\n",
       "       [776.    ,   0.7821,   1.    ],\n",
       "       [568.    ,   0.7834,   1.    ],\n",
       "       [  8.    ,   0.7849,   1.    ],\n",
       "       [895.    ,   0.7861,   1.    ],\n",
       "       [497.    ,   0.787 ,   1.    ],\n",
       "       [ 70.    ,   0.7908,   1.    ],\n",
       "       [681.    ,   0.7926,   1.    ],\n",
       "       [197.    ,   0.7937,   1.    ],\n",
       "       [141.    ,   0.794 ,   1.    ],\n",
       "       [473.    ,   0.796 ,   1.    ],\n",
       "       [490.    ,   0.7979,   1.    ],\n",
       "       [418.    ,   0.7991,   1.    ],\n",
       "       [455.    ,   0.7996,   1.    ],\n",
       "       [842.    ,   0.7999,   1.    ],\n",
       "       [734.    ,   0.8006,   1.    ],\n",
       "       [517.    ,   0.8016,   1.    ],\n",
       "       [363.    ,   0.8031,   1.    ],\n",
       "       [773.    ,   0.8036,   1.    ],\n",
       "       [ 71.    ,   0.8039,   1.    ],\n",
       "       [672.    ,   0.8043,   1.    ],\n",
       "       [636.    ,   0.8048,   1.    ],\n",
       "       [707.    ,   0.8101,   1.    ],\n",
       "       [897.    ,   0.8108,   1.    ],\n",
       "       [100.    ,   0.8136,   1.    ],\n",
       "       [664.    ,   0.8157,   1.    ],\n",
       "       [686.    ,   0.8216,   1.    ],\n",
       "       [530.    ,   0.8219,   1.    ],\n",
       "       [355.    ,   0.8244,   1.    ],\n",
       "       [806.    ,   0.8249,   1.    ],\n",
       "       [609.    ,   0.8251,   1.    ],\n",
       "       [  0.    ,   0.8251,   1.    ],\n",
       "       [154.    ,   0.8253,   1.    ],\n",
       "       [348.    ,   0.8267,   1.    ],\n",
       "       [104.    ,   0.828 ,   1.    ],\n",
       "       [498.    ,   0.8287,   1.    ],\n",
       "       [673.    ,   0.8289,   1.    ],\n",
       "       [124.    ,   0.8293,   1.    ],\n",
       "       [352.    ,   0.831 ,   1.    ],\n",
       "       [551.    ,   0.8318,   1.    ],\n",
       "       [508.    ,   0.8319,   1.    ],\n",
       "       [250.    ,   0.8319,   1.    ],\n",
       "       [756.    ,   0.8332,   1.    ],\n",
       "       [761.    ,   0.8337,   1.    ],\n",
       "       [702.    ,   0.8339,   1.    ],\n",
       "       [535.    ,   0.8354,   1.    ],\n",
       "       [808.    ,   0.8371,   1.    ],\n",
       "       [162.    ,   0.8379,   1.    ],\n",
       "       [620.    ,   0.8413,   1.    ],\n",
       "       [561.    ,   0.8414,   1.    ],\n",
       "       [610.    ,   0.8418,   1.    ],\n",
       "       [651.    ,   0.8424,   1.    ],\n",
       "       [228.    ,   0.8431,   1.    ],\n",
       "       [420.    ,   0.8437,   1.    ],\n",
       "       [117.    ,   0.8438,   1.    ],\n",
       "       [757.    ,   0.8445,   1.    ],\n",
       "       [804.    ,   0.8454,   1.    ],\n",
       "       [814.    ,   0.846 ,   1.    ],\n",
       "       [529.    ,   0.8488,   1.    ],\n",
       "       [434.    ,   0.8495,   1.    ],\n",
       "       [243.    ,   0.8523,   1.    ],\n",
       "       [424.    ,   0.8544,   1.    ],\n",
       "       [545.    ,   0.856 ,   1.    ],\n",
       "       [594.    ,   0.8568,   1.    ],\n",
       "       [180.    ,   0.8579,   1.    ],\n",
       "       [759.    ,   0.859 ,   1.    ],\n",
       "       [892.    ,   0.861 ,   1.    ],\n",
       "       [890.    ,   0.8642,   1.    ],\n",
       "       [287.    ,   0.8646,   1.    ],\n",
       "       [330.    ,   0.8655,   1.    ],\n",
       "       [221.    ,   0.8679,   1.    ],\n",
       "       [721.    ,   0.8682,   1.    ],\n",
       "       [625.    ,   0.8695,   1.    ],\n",
       "       [372.    ,   0.8702,   1.    ],\n",
       "       [388.    ,   0.8712,   1.    ],\n",
       "       [409.    ,   0.8726,   1.    ],\n",
       "       [187.    ,   0.873 ,   1.    ],\n",
       "       [357.    ,   0.8758,   1.    ],\n",
       "       [633.    ,   0.8758,   1.    ],\n",
       "       [320.    ,   0.8758,   1.    ],\n",
       "       [ 12.    ,   0.879 ,   1.    ],\n",
       "       [798.    ,   0.8792,   1.    ],\n",
       "       [506.    ,   0.8794,   1.    ],\n",
       "       [656.    ,   0.8801,   1.    ],\n",
       "       [569.    ,   0.8812,   1.    ],\n",
       "       [283.    ,   0.8818,   1.    ],\n",
       "       [373.    ,   0.8824,   1.    ],\n",
       "       [396.    ,   0.885 ,   1.    ],\n",
       "       [700.    ,   0.8879,   1.    ],\n",
       "       [472.    ,   0.8913,   1.    ],\n",
       "       [ 85.    ,   0.8928,   1.    ],\n",
       "       [752.    ,   0.8933,   1.    ],\n",
       "       [299.    ,   0.8946,   1.    ],\n",
       "       [813.    ,   0.8955,   1.    ],\n",
       "       [ 93.    ,   0.9044,   1.    ],\n",
       "       [489.    ,   0.905 ,   1.    ],\n",
       "       [224.    ,   0.9063,   1.    ],\n",
       "       [851.    ,   0.9075,   1.    ],\n",
       "       [286.    ,   0.9118,   1.    ],\n",
       "       [266.    ,   0.9136,   1.    ],\n",
       "       [191.    ,   0.9156,   1.    ],\n",
       "       [401.    ,   0.9168,   1.    ],\n",
       "       [ 19.    ,   0.9191,   1.    ],\n",
       "       [873.    ,   0.9212,   1.    ],\n",
       "       [738.    ,   0.9225,   1.    ],\n",
       "       [755.    ,   0.9228,   1.    ],\n",
       "       [109.    ,   0.9241,   1.    ],\n",
       "       [202.    ,   0.9253,   1.    ],\n",
       "       [267.    ,   0.9259,   1.    ],\n",
       "       [ 31.    ,   0.926 ,   1.    ],\n",
       "       [115.    ,   0.9265,   1.    ],\n",
       "       [391.    ,   0.9267,   1.    ],\n",
       "       [500.    ,   0.9313,   1.    ],\n",
       "       [168.    ,   0.932 ,   1.    ],\n",
       "       [709.    ,   0.9352,   1.    ],\n",
       "       [ 79.    ,   0.9353,   1.    ],\n",
       "       [531.    ,   0.9355,   1.    ],\n",
       "       [309.    ,   0.9357,   1.    ],\n",
       "       [ 68.    ,   0.937 ,   1.    ],\n",
       "       [ 78.    ,   0.9371,   1.    ],\n",
       "       [748.    ,   0.9376,   1.    ],\n",
       "       [323.    ,   0.9378,   1.    ],\n",
       "       [322.    ,   0.938 ,   1.    ],\n",
       "       [ 23.    ,   0.9381,   1.    ],\n",
       "       [547.    ,   0.939 ,   1.    ],\n",
       "       [548.    ,   0.94  ,   1.    ],\n",
       "       [173.    ,   0.9416,   1.    ],\n",
       "       [590.    ,   0.9418,   1.    ],\n",
       "       [ 33.    ,   0.9428,   1.    ],\n",
       "       [304.    ,   0.9441,   1.    ],\n",
       "       [864.    ,   0.9462,   1.    ],\n",
       "       [140.    ,   0.9469,   1.    ],\n",
       "       [237.    ,   0.9481,   1.    ],\n",
       "       [ 91.    ,   0.9491,   1.    ],\n",
       "       [240.    ,   0.9516,   1.    ],\n",
       "       [693.    ,   0.9525,   1.    ],\n",
       "       [209.    ,   0.953 ,   1.    ],\n",
       "       [333.    ,   0.9556,   1.    ],\n",
       "       [621.    ,   0.9557,   1.    ],\n",
       "       [750.    ,   0.957 ,   1.    ],\n",
       "       [722.    ,   0.9577,   1.    ],\n",
       "       [789.    ,   0.9581,   1.    ],\n",
       "       [254.    ,   0.9589,   1.    ],\n",
       "       [274.    ,   0.96  ,   1.    ],\n",
       "       [730.    ,   0.9626,   1.    ],\n",
       "       [470.    ,   0.9631,   1.    ],\n",
       "       [461.    ,   0.9638,   1.    ],\n",
       "       [126.    ,   0.9649,   1.    ],\n",
       "       [468.    ,   0.9684,   1.    ],\n",
       "       [ 49.    ,   0.9701,   1.    ],\n",
       "       [128.    ,   0.9707,   1.    ],\n",
       "       [110.    ,   0.9712,   1.    ],\n",
       "       [658.    ,   0.9724,   1.    ],\n",
       "       [839.    ,   0.9736,   1.    ],\n",
       "       [826.    ,   0.9755,   1.    ],\n",
       "       [247.    ,   0.976 ,   1.    ],\n",
       "       [354.    ,   0.9763,   1.    ],\n",
       "       [532.    ,   0.977 ,   1.    ],\n",
       "       [157.    ,   0.9783,   1.    ],\n",
       "       [745.    ,   0.9829,   1.    ],\n",
       "       [605.    ,   0.9831,   1.    ],\n",
       "       [261.    ,   0.9837,   1.    ],\n",
       "       [875.    ,   0.984 ,   1.    ],\n",
       "       [  6.    ,   0.9843,   1.    ],\n",
       "       [273.    ,   0.9853,   1.    ],\n",
       "       [406.    ,   0.9872,   1.    ],\n",
       "       [571.    ,   0.9875,   1.    ],\n",
       "       [670.    ,   0.9881,   1.    ],\n",
       "       [344.    ,   0.9904,   1.    ],\n",
       "       [810.    ,   0.9906,   1.    ],\n",
       "       [744.    ,   0.9909,   1.    ],\n",
       "       [118.    ,   0.9911,   1.    ],\n",
       "       [615.    ,   0.9949,   1.    ],\n",
       "       [725.    ,   0.9957,   1.    ],\n",
       "       [ 28.    ,   0.9971,   1.    ],\n",
       "       [436.    ,   0.9971,   1.    ],\n",
       "       [ 60.    ,   0.9985,   1.    ],\n",
       "       [294.    ,   0.999 ,   1.    ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the prediction values\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9aa541",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c45d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df= pd.read_csv(\"face_data.csv\", usecols=[\"Image Path\",\"Face\",\"Bounding Box\",\"Target\"]) \n",
    "eval_df=eval_df[eval_df[\"Face\"]==1]\n",
    "# display(eval_df.head())\n",
    "\n",
    "results=[]\n",
    "for i in tqdm(eval_df.iterrows()):\n",
    "    img=cv2.imread(i[1][\"Image Path\"])\n",
    "    img=cv2.resize(img, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
    "    bb=eval(i[1][\"Bounding Box\"])\n",
    "    crp_img=img[bb[1]:bb[3],bb[0]:bb[2]]\n",
    "    result=fr.detect(crp_img)\n",
    "    cls=int(torch.argmax(result,dim=1))\n",
    "    cls=902 if cls==901 else cls\n",
    "    if cls:results.append(cls)\n",
    "    else:results.append(None)  \n",
    "eval_df[\"Predicted\"]=results \n",
    "display(eval_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847bdd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Confusion Matrix===================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  3,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0, 12, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  4,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  4,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  8]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "final=eval_df.dropna()\n",
    "y_true = final[\"Target\"].tolist()\n",
    "y_pred = final[\"Predicted\"].tolist()\n",
    "print(\"============== Confusion Matrix===================\")\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed4213d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative:0, True positive: 1, False negative:1, False positive:2\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "print(f\"True negative:{tn}, True positive: {tp}, False negative:{fn}, False positive:{fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3a853fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++Classification Report++++++++++++++++++++++++\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      0.92      0.96        13\n",
      "           4       0.33      1.00      0.50         5\n",
      "           5       0.33      1.00      0.50         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       0.90      1.00      0.95         9\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       1.00      0.25      0.40         4\n",
      "          10       1.00      0.89      0.94         9\n",
      "          11       0.88      1.00      0.93         7\n",
      "          12       0.75      1.00      0.86         6\n",
      "          13       0.90      1.00      0.95         9\n",
      "          14       1.00      0.33      0.50         3\n",
      "          15       1.00      1.00      1.00         4\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         7\n",
      "          18       1.00      1.00      1.00         7\n",
      "          19       1.00      1.00      1.00         3\n",
      "          20       1.00      0.33      0.50         6\n",
      "          21       1.00      0.60      0.75         5\n",
      "          22       1.00      1.00      1.00         3\n",
      "          23       1.00      0.86      0.92         7\n",
      "          24       0.83      1.00      0.91         5\n",
      "          25       1.00      1.00      1.00         4\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         4\n",
      "          28       0.92      1.00      0.96        12\n",
      "          29       0.40      1.00      0.58        21\n",
      "          30       1.00      1.00      1.00         4\n",
      "          31       0.75      0.75      0.75         4\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00        10\n",
      "          34       1.00      1.00      1.00         4\n",
      "          35       1.00      1.00      1.00         3\n",
      "          36       1.00      0.93      0.96        14\n",
      "          37       1.00      1.00      1.00        10\n",
      "          38       1.00      1.00      1.00         4\n",
      "          39       1.00      1.00      1.00        10\n",
      "          40       1.00      0.67      0.80         6\n",
      "          41       1.00      0.60      0.75        10\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       1.00      1.00      1.00         3\n",
      "          44       0.50      1.00      0.67         4\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      0.67      0.80         3\n",
      "          47       1.00      1.00      1.00         4\n",
      "          48       1.00      1.00      1.00         3\n",
      "          49       1.00      0.67      0.80         3\n",
      "          50       1.00      0.80      0.89         5\n",
      "          51       0.50      1.00      0.67         4\n",
      "          52       1.00      1.00      1.00         4\n",
      "          53       0.85      1.00      0.92        11\n",
      "          54       1.00      0.75      0.86         4\n",
      "          55       1.00      1.00      1.00         5\n",
      "          56       1.00      0.33      0.50         3\n",
      "          57       0.67      0.67      0.67         3\n",
      "          58       1.00      0.80      0.89         5\n",
      "          59       1.00      1.00      1.00         3\n",
      "          60       1.00      0.76      0.87        17\n",
      "          61       1.00      0.88      0.93         8\n",
      "          62       1.00      1.00      1.00         3\n",
      "          63       1.00      1.00      1.00         9\n",
      "          64       1.00      1.00      1.00         4\n",
      "          65       0.75      1.00      0.86         3\n",
      "          66       0.86      1.00      0.92         6\n",
      "          67       1.00      0.83      0.91        18\n",
      "          68       0.75      1.00      0.86         3\n",
      "          69       1.00      0.80      0.89         5\n",
      "          70       0.47      0.90      0.62        10\n",
      "          71       1.00      0.88      0.94        17\n",
      "          72       1.00      1.00      1.00         3\n",
      "          73       1.00      0.50      0.67         8\n",
      "          74       1.00      1.00      1.00         3\n",
      "          75       1.00      1.00      1.00         4\n",
      "          76       1.00      0.60      0.75         5\n",
      "          77       0.62      1.00      0.77         5\n",
      "          78       1.00      0.96      0.98        25\n",
      "          79       0.90      1.00      0.95         9\n",
      "          80       1.00      1.00      1.00         5\n",
      "          81       0.92      1.00      0.96        35\n",
      "          82       0.69      0.90      0.78        10\n",
      "          83       1.00      0.90      0.95        20\n",
      "          84       0.75      1.00      0.86         3\n",
      "          85       1.00      1.00      1.00         7\n",
      "          86       1.00      1.00      1.00         5\n",
      "          87       1.00      0.67      0.80         3\n",
      "          88       1.00      1.00      1.00         3\n",
      "          89       1.00      1.00      1.00        17\n",
      "          90       1.00      1.00      1.00         7\n",
      "          91       1.00      0.75      0.86         4\n",
      "          92       1.00      0.67      0.80         3\n",
      "          93       1.00      0.75      0.86         4\n",
      "          94       0.75      1.00      0.86         3\n",
      "          95       1.00      1.00      1.00         8\n",
      "          96       1.00      1.00      1.00        10\n",
      "          97       1.00      1.00      1.00        11\n",
      "          98       0.00      0.00      0.00         3\n",
      "          99       1.00      1.00      1.00         3\n",
      "         100       1.00      1.00      1.00         3\n",
      "         101       1.00      1.00      1.00         3\n",
      "         102       1.00      1.00      1.00         3\n",
      "         103       1.00      1.00      1.00         6\n",
      "         104       1.00      1.00      1.00         3\n",
      "         105       1.00      0.60      0.75         5\n",
      "         106       1.00      1.00      1.00         4\n",
      "         107       1.00      0.67      0.80         3\n",
      "         108       1.00      0.50      0.67         2\n",
      "         109       1.00      1.00      1.00         3\n",
      "         110       1.00      1.00      1.00         3\n",
      "         111       1.00      1.00      1.00        15\n",
      "         112       1.00      1.00      1.00         3\n",
      "         113       0.21      1.00      0.35         4\n",
      "         114       1.00      1.00      1.00         3\n",
      "         115       1.00      0.43      0.60         7\n",
      "         116       1.00      1.00      1.00         4\n",
      "         117       0.75      1.00      0.86         3\n",
      "         118       1.00      1.00      1.00         3\n",
      "         119       1.00      1.00      1.00         3\n",
      "         120       0.83      1.00      0.91         5\n",
      "         121       0.27      1.00      0.43         3\n",
      "         122       0.43      1.00      0.60         3\n",
      "         123       1.00      0.75      0.86         4\n",
      "         124       1.00      1.00      1.00         4\n",
      "         125       1.00      1.00      1.00         3\n",
      "         126       0.83      1.00      0.91         5\n",
      "         127       1.00      1.00      1.00         9\n",
      "         128       1.00      0.83      0.91        18\n",
      "         129       1.00      1.00      1.00         6\n",
      "         130       1.00      1.00      1.00         4\n",
      "         131       0.67      1.00      0.80         4\n",
      "         132       1.00      0.36      0.53        14\n",
      "         133       1.00      1.00      1.00         3\n",
      "         134       1.00      0.94      0.97        16\n",
      "         135       1.00      0.40      0.57         5\n",
      "         136       1.00      1.00      1.00         3\n",
      "         137       1.00      1.00      1.00         4\n",
      "         138       1.00      1.00      1.00         3\n",
      "         139       1.00      1.00      1.00         3\n",
      "         140       1.00      0.80      0.89         5\n",
      "         141       0.50      1.00      0.67         3\n",
      "         142       1.00      0.89      0.94         9\n",
      "         143       1.00      1.00      1.00         7\n",
      "         144       0.75      1.00      0.86         3\n",
      "         145       0.75      1.00      0.86         3\n",
      "         146       1.00      0.67      0.80         3\n",
      "         147       0.96      0.96      0.96        27\n",
      "         148       1.00      1.00      1.00         3\n",
      "         149       1.00      0.90      0.95        10\n",
      "         150       1.00      0.29      0.44         7\n",
      "         151       1.00      0.95      0.97        20\n",
      "         152       1.00      1.00      1.00         5\n",
      "         153       1.00      0.75      0.86         4\n",
      "         154       1.00      1.00      1.00         3\n",
      "         155       1.00      0.14      0.25         7\n",
      "         156       1.00      0.75      0.86         4\n",
      "         157       1.00      1.00      1.00         3\n",
      "         158       1.00      0.67      0.80         3\n",
      "         159       1.00      0.92      0.96        13\n",
      "         160       1.00      1.00      1.00        20\n",
      "         161       0.80      0.80      0.80         5\n",
      "         162       1.00      0.80      0.89         5\n",
      "         163       1.00      1.00      1.00        10\n",
      "         164       1.00      1.00      1.00         4\n",
      "         165       1.00      0.67      0.80         3\n",
      "         166       1.00      1.00      1.00         5\n",
      "         167       0.88      1.00      0.93         7\n",
      "         168       1.00      1.00      1.00         4\n",
      "         169       1.00      1.00      1.00         4\n",
      "         170       1.00      0.97      0.98        31\n",
      "         171       1.00      0.80      0.89         5\n",
      "         172       0.92      1.00      0.96        12\n",
      "         173       1.00      1.00      1.00         6\n",
      "         174       1.00      1.00      1.00         4\n",
      "         175       1.00      1.00      1.00         4\n",
      "         176       0.67      0.67      0.67         3\n",
      "         177       1.00      1.00      1.00         3\n",
      "         178       0.83      0.83      0.83         6\n",
      "         179       0.56      1.00      0.71         5\n",
      "         180       1.00      0.50      0.67         4\n",
      "         181       1.00      1.00      1.00         4\n",
      "         182       1.00      1.00      1.00         3\n",
      "         183       1.00      1.00      1.00         4\n",
      "         184       1.00      1.00      1.00         8\n",
      "         185       0.62      1.00      0.77         5\n",
      "         186       1.00      1.00      1.00         4\n",
      "         187       0.75      1.00      0.86         3\n",
      "         188       1.00      0.50      0.67         4\n",
      "         189       1.00      1.00      1.00         4\n",
      "         190       1.00      1.00      1.00        10\n",
      "         191       0.93      1.00      0.97        14\n",
      "         192       1.00      1.00      1.00         5\n",
      "         193       1.00      0.95      0.97        19\n",
      "         194       1.00      1.00      1.00         4\n",
      "         195       0.83      1.00      0.91         5\n",
      "         196       0.80      0.67      0.73         6\n",
      "         197       1.00      1.00      1.00         3\n",
      "         198       1.00      1.00      1.00         4\n",
      "         199       1.00      0.80      0.89         5\n",
      "         200       1.00      1.00      1.00        50\n",
      "         201       1.00      0.67      0.80         3\n",
      "         202       1.00      0.67      0.80         3\n",
      "         203       1.00      0.93      0.97        15\n",
      "         204       1.00      0.71      0.83         7\n",
      "         205       1.00      1.00      1.00         7\n",
      "         206       1.00      1.00      1.00         3\n",
      "         207       1.00      1.00      1.00         3\n",
      "         208       1.00      0.67      0.80         3\n",
      "         209       1.00      0.57      0.73         7\n",
      "         210       1.00      1.00      1.00         4\n",
      "         211       1.00      1.00      1.00         3\n",
      "         212       0.96      0.96      0.96        25\n",
      "         213       1.00      1.00      1.00         3\n",
      "         214       1.00      0.82      0.90       109\n",
      "         215       1.00      1.00      1.00         7\n",
      "         216       0.83      1.00      0.91         5\n",
      "         217       0.50      0.33      0.40         3\n",
      "         218       1.00      0.25      0.40         4\n",
      "         219       1.00      1.00      1.00         7\n",
      "         220       1.00      0.83      0.91         6\n",
      "         221       1.00      0.75      0.86         8\n",
      "         222       1.00      1.00      1.00        23\n",
      "         223       1.00      0.94      0.97        32\n",
      "         224       1.00      1.00      1.00         3\n",
      "         225       1.00      1.00      1.00         9\n",
      "         226       0.50      1.00      0.67         3\n",
      "         227       1.00      0.90      0.95        21\n",
      "         228       1.00      1.00      1.00         3\n",
      "         229       1.00      1.00      1.00         3\n",
      "         230       0.80      1.00      0.89         4\n",
      "         231       1.00      1.00      1.00         6\n",
      "         232       0.92      1.00      0.96        49\n",
      "         233       0.91      1.00      0.95        21\n",
      "         234       1.00      1.00      1.00        15\n",
      "         235       1.00      1.00      1.00         3\n",
      "         236       1.00      1.00      1.00         3\n",
      "         237       0.75      1.00      0.86         3\n",
      "         238       1.00      0.67      0.80         3\n",
      "         239       1.00      0.84      0.91       144\n",
      "         240       1.00      1.00      1.00         3\n",
      "         241       1.00      1.00      1.00         5\n",
      "         242       1.00      0.25      0.40         4\n",
      "         243       0.80      1.00      0.89         4\n",
      "         244       1.00      1.00      1.00         4\n",
      "         245       1.00      0.91      0.95        11\n",
      "         246       1.00      1.00      1.00         6\n",
      "         247       1.00      1.00      1.00         3\n",
      "         248       1.00      0.80      0.89         5\n",
      "         249       1.00      1.00      1.00         4\n",
      "         250       1.00      0.67      0.80         3\n",
      "         251       0.86      1.00      0.92         6\n",
      "         252       1.00      0.90      0.95        10\n",
      "         253       1.00      1.00      1.00         6\n",
      "         254       1.00      0.80      0.89         5\n",
      "         255       0.90      1.00      0.95        28\n",
      "         256       0.92      1.00      0.96        22\n",
      "         257       0.73      1.00      0.85        11\n",
      "         258       1.00      0.75      0.86         4\n",
      "         259       0.86      1.00      0.92         6\n",
      "         260       1.00      0.67      0.80         3\n",
      "         261       1.00      1.00      1.00         5\n",
      "         262       0.67      0.67      0.67         6\n",
      "         263       1.00      0.90      0.95        20\n",
      "         264       0.81      1.00      0.89        29\n",
      "         265       1.00      0.75      0.86         4\n",
      "         266       1.00      1.00      1.00         3\n",
      "         267       1.00      1.00      1.00         3\n",
      "         268       1.00      0.75      0.86         4\n",
      "         269       1.00      0.75      0.86         4\n",
      "         270       1.00      1.00      1.00         3\n",
      "         271       0.89      1.00      0.94         8\n",
      "         272       0.67      1.00      0.80         4\n",
      "         273       1.00      1.00      1.00         3\n",
      "         274       1.00      1.00      1.00         3\n",
      "         275       1.00      0.94      0.97        18\n",
      "         276       0.60      1.00      0.75         3\n",
      "         277       1.00      0.60      0.75         5\n",
      "         278       1.00      1.00      1.00         4\n",
      "         279       1.00      0.67      0.80        12\n",
      "         280       1.00      1.00      1.00         4\n",
      "         281       1.00      1.00      1.00         4\n",
      "         282       1.00      0.65      0.79        37\n",
      "         283       1.00      1.00      1.00         4\n",
      "         284       1.00      0.67      0.80         3\n",
      "         285       1.00      1.00      1.00         3\n",
      "         286       1.00      1.00      1.00         3\n",
      "         287       0.86      1.00      0.92         6\n",
      "         288       1.00      0.67      0.80         3\n",
      "         289       1.00      0.75      0.86         4\n",
      "         290       1.00      1.00      1.00         6\n",
      "         291       0.60      1.00      0.75         3\n",
      "         292       1.00      1.00      1.00         3\n",
      "         293       1.00      1.00      1.00         7\n",
      "         294       1.00      0.33      0.50         3\n",
      "         295       1.00      1.00      1.00         4\n",
      "         296       1.00      1.00      1.00         3\n",
      "         297       1.00      1.00      1.00         3\n",
      "         298       1.00      1.00      1.00         3\n",
      "         299       0.75      1.00      0.86         3\n",
      "         300       1.00      0.33      0.50         3\n",
      "         301       0.89      1.00      0.94         8\n",
      "         302       1.00      0.80      0.89         5\n",
      "         303       1.00      1.00      1.00         3\n",
      "         304       0.75      1.00      0.86         3\n",
      "         305       0.82      0.90      0.86        10\n",
      "         306       1.00      1.00      1.00         5\n",
      "         307       0.95      0.82      0.88        22\n",
      "         308       1.00      0.67      0.80         3\n",
      "         309       1.00      1.00      1.00         3\n",
      "         310       1.00      1.00      1.00         4\n",
      "         311       1.00      1.00      1.00         4\n",
      "         312       0.64      1.00      0.78        18\n",
      "         313       1.00      0.92      0.96        12\n",
      "         314       1.00      0.67      0.80         6\n",
      "         315       0.80      1.00      0.89         4\n",
      "         316       1.00      1.00      1.00         5\n",
      "         317       1.00      0.75      0.86         4\n",
      "         318       1.00      1.00      1.00         3\n",
      "         319       1.00      1.00      1.00         3\n",
      "         320       1.00      0.67      0.80         3\n",
      "         321       1.00      0.25      0.40         4\n",
      "         322       1.00      0.80      0.89         5\n",
      "         323       1.00      0.94      0.97        17\n",
      "         324       0.60      1.00      0.75         6\n",
      "         325       1.00      1.00      1.00         5\n",
      "         326       1.00      1.00      1.00         3\n",
      "         327       1.00      1.00      1.00         4\n",
      "         328       1.00      0.92      0.96        13\n",
      "         329       1.00      0.75      0.86         4\n",
      "         330       1.00      1.00      1.00         8\n",
      "         331       1.00      1.00      1.00         3\n",
      "         332       1.00      1.00      1.00         4\n",
      "         333       0.86      1.00      0.92         6\n",
      "         334       1.00      0.50      0.67         4\n",
      "         335       0.67      0.67      0.67         3\n",
      "         336       1.00      0.67      0.80         3\n",
      "         337       1.00      1.00      1.00         3\n",
      "         338       1.00      0.91      0.95        32\n",
      "         339       1.00      0.90      0.95        10\n",
      "         340       1.00      1.00      1.00         3\n",
      "         341       1.00      0.67      0.80         3\n",
      "         342       0.87      1.00      0.93        48\n",
      "         343       0.60      1.00      0.75         3\n",
      "         344       1.00      1.00      1.00         5\n",
      "         345       1.00      0.89      0.94         9\n",
      "         346       1.00      1.00      1.00         3\n",
      "         347       1.00      0.75      0.86         4\n",
      "         348       0.88      1.00      0.93        14\n",
      "         349       0.67      1.00      0.80         6\n",
      "         350       0.75      0.75      0.75         4\n",
      "         351       1.00      1.00      1.00         3\n",
      "         352       1.00      0.67      0.80         3\n",
      "         353       0.00      0.00      0.00         3\n",
      "         354       0.95      0.95      0.95        20\n",
      "         355       1.00      0.50      0.67         4\n",
      "         356       0.41      1.00      0.58         9\n",
      "         357       1.00      1.00      1.00         3\n",
      "         358       1.00      0.50      0.67         4\n",
      "         359       1.00      1.00      1.00         5\n",
      "         360       1.00      1.00      1.00         4\n",
      "         361       1.00      1.00      1.00         3\n",
      "         362       1.00      1.00      1.00         4\n",
      "         363       1.00      0.67      0.80         3\n",
      "         364       1.00      0.83      0.91         6\n",
      "         365       1.00      1.00      1.00         5\n",
      "         366       1.00      1.00      1.00         4\n",
      "         367       1.00      1.00      1.00         6\n",
      "         368       1.00      1.00      1.00         4\n",
      "         369       0.95      1.00      0.98        21\n",
      "         370       0.67      1.00      0.80         6\n",
      "         371       1.00      0.82      0.90        11\n",
      "         372       1.00      0.67      0.80         3\n",
      "         373       1.00      0.67      0.80         3\n",
      "         374       1.00      0.67      0.80         3\n",
      "         375       1.00      0.80      0.89         5\n",
      "         376       0.67      0.50      0.57         4\n",
      "         377       1.00      1.00      1.00         3\n",
      "         378       1.00      1.00      1.00         3\n",
      "         379       1.00      1.00      1.00         3\n",
      "         380       1.00      1.00      1.00         3\n",
      "         381       1.00      1.00      1.00         3\n",
      "         382       1.00      1.00      1.00         5\n",
      "         383       0.40      1.00      0.57         4\n",
      "         384       0.75      1.00      0.86         3\n",
      "         385       1.00      1.00      1.00         3\n",
      "         386       0.75      1.00      0.86         3\n",
      "         387       0.92      1.00      0.96        11\n",
      "         388       0.67      0.67      0.67         3\n",
      "         389       1.00      1.00      1.00         5\n",
      "         390       1.00      1.00      1.00        10\n",
      "         391       1.00      1.00      1.00         3\n",
      "         392       1.00      1.00      1.00         3\n",
      "         393       0.67      1.00      0.80         4\n",
      "         394       1.00      1.00      1.00         8\n",
      "         395       1.00      1.00      1.00         4\n",
      "         396       1.00      1.00      1.00         6\n",
      "         397       1.00      1.00      1.00         3\n",
      "         398       1.00      1.00      1.00         5\n",
      "         399       1.00      1.00      1.00         4\n",
      "         400       1.00      1.00      1.00         5\n",
      "         401       1.00      1.00      1.00         3\n",
      "         402       1.00      1.00      1.00         5\n",
      "         403       1.00      1.00      1.00         3\n",
      "         404       1.00      1.00      1.00         3\n",
      "         405       0.75      1.00      0.86         6\n",
      "         406       1.00      0.64      0.78        14\n",
      "         407       1.00      1.00      1.00         4\n",
      "         408       1.00      1.00      1.00         3\n",
      "         409       1.00      1.00      1.00         5\n",
      "         410       0.70      0.88      0.78         8\n",
      "         411       1.00      1.00      1.00         7\n",
      "         412       0.60      1.00      0.75         6\n",
      "         413       0.72      1.00      0.84        13\n",
      "         414       0.50      1.00      0.67         3\n",
      "         415       1.00      1.00      1.00         5\n",
      "         416       1.00      1.00      1.00         3\n",
      "         417       1.00      1.00      1.00         3\n",
      "         418       0.83      1.00      0.91         5\n",
      "         419       1.00      0.50      0.67         4\n",
      "         420       1.00      1.00      1.00         4\n",
      "         421       1.00      1.00      1.00         3\n",
      "         422       1.00      1.00      1.00         3\n",
      "         423       0.67      0.50      0.57         4\n",
      "         424       1.00      0.75      0.86         4\n",
      "         425       0.75      1.00      0.86         3\n",
      "         426       1.00      1.00      1.00         6\n",
      "         427       1.00      1.00      1.00         3\n",
      "         428       0.75      1.00      0.86         3\n",
      "         429       0.89      1.00      0.94        24\n",
      "         430       1.00      1.00      1.00        11\n",
      "         431       0.50      1.00      0.67         5\n",
      "         432       1.00      0.60      0.75         5\n",
      "         433       1.00      0.97      0.98        33\n",
      "         434       1.00      0.71      0.83         7\n",
      "         435       1.00      0.67      0.80         3\n",
      "         436       0.40      0.67      0.50         3\n",
      "         437       0.60      0.75      0.67         4\n",
      "         438       1.00      0.50      0.67         4\n",
      "         439       1.00      1.00      1.00         4\n",
      "         440       1.00      0.82      0.90        55\n",
      "         441       1.00      1.00      1.00         3\n",
      "         442       1.00      1.00      1.00         5\n",
      "         443       1.00      1.00      1.00         3\n",
      "         444       1.00      1.00      1.00         4\n",
      "         445       1.00      0.80      0.89         5\n",
      "         446       1.00      1.00      1.00         8\n",
      "         447       1.00      1.00      1.00         3\n",
      "         448       1.00      1.00      1.00         6\n",
      "         449       1.00      0.87      0.93        15\n",
      "         450       1.00      1.00      1.00         2\n",
      "         451       1.00      1.00      1.00         3\n",
      "         452       1.00      0.67      0.80         3\n",
      "         453       1.00      0.94      0.97        36\n",
      "         454       1.00      1.00      1.00         8\n",
      "         455       1.00      0.83      0.91         6\n",
      "         456       0.75      1.00      0.86         3\n",
      "         457       1.00      0.75      0.86         4\n",
      "         458       1.00      1.00      1.00         3\n",
      "         459       1.00      1.00      1.00         4\n",
      "         460       0.50      1.00      0.67        17\n",
      "         461       0.79      0.97      0.87        31\n",
      "         462       0.99      1.00      1.00       120\n",
      "         463       1.00      1.00      1.00         3\n",
      "         464       1.00      1.00      1.00         3\n",
      "         465       1.00      1.00      1.00         3\n",
      "         466       1.00      1.00      1.00         4\n",
      "         467       1.00      0.93      0.96        41\n",
      "         468       0.83      0.95      0.89        41\n",
      "         469       1.00      0.38      0.55         8\n",
      "         470       1.00      1.00      1.00        10\n",
      "         471       1.00      0.78      0.88         9\n",
      "         472       1.00      1.00      1.00        13\n",
      "         473       1.00      0.75      0.86         4\n",
      "         474       1.00      1.00      1.00         7\n",
      "         475       0.00      0.00      0.00         4\n",
      "         476       1.00      1.00      1.00         3\n",
      "         477       1.00      0.25      0.40         4\n",
      "         478       0.80      1.00      0.89         4\n",
      "         479       1.00      1.00      1.00         7\n",
      "         480       0.60      1.00      0.75         3\n",
      "         481       1.00      0.50      0.67         8\n",
      "         482       0.86      1.00      0.92         6\n",
      "         483       1.00      1.00      1.00         3\n",
      "         484       1.00      1.00      1.00         6\n",
      "         485       1.00      1.00      1.00         4\n",
      "         486       1.00      1.00      1.00         3\n",
      "         487       1.00      0.80      0.89         5\n",
      "         488       0.67      0.67      0.67         3\n",
      "         489       1.00      0.75      0.86         4\n",
      "         490       1.00      0.67      0.80         3\n",
      "         491       0.60      1.00      0.75         3\n",
      "         492       1.00      1.00      1.00         3\n",
      "         493       1.00      1.00      1.00        28\n",
      "         494       1.00      1.00      1.00         3\n",
      "         495       1.00      1.00      1.00         4\n",
      "         496       1.00      0.75      0.86         4\n",
      "         497       0.67      1.00      0.80         4\n",
      "         498       0.75      1.00      0.86         3\n",
      "         499       1.00      1.00      1.00         4\n",
      "         500       1.00      0.33      0.50         3\n",
      "         501       1.00      1.00      1.00         3\n",
      "         502       0.92      0.92      0.92        12\n",
      "         503       1.00      1.00      1.00         5\n",
      "         504       1.00      0.67      0.80         3\n",
      "         505       1.00      0.25      0.40         4\n",
      "         506       1.00      0.89      0.94         9\n",
      "         507       1.00      1.00      1.00         4\n",
      "         508       1.00      1.00      1.00         6\n",
      "         509       1.00      0.75      0.86         4\n",
      "         510       0.83      1.00      0.91         5\n",
      "         511       1.00      0.25      0.40         4\n",
      "         512       0.75      0.67      0.71         9\n",
      "         513       1.00      1.00      1.00         4\n",
      "         514       1.00      0.80      0.89         5\n",
      "         515       0.80      1.00      0.89         4\n",
      "         516       1.00      0.20      0.33         5\n",
      "         517       0.83      1.00      0.91         5\n",
      "         518       1.00      1.00      1.00         3\n",
      "         519       0.43      1.00      0.60         3\n",
      "         520       1.00      0.75      0.86         4\n",
      "         521       1.00      1.00      1.00         4\n",
      "         522       1.00      0.75      0.86         4\n",
      "         523       0.88      0.88      0.88         8\n",
      "         524       1.00      1.00      1.00         3\n",
      "         525       0.82      1.00      0.90         9\n",
      "         526       1.00      1.00      1.00         6\n",
      "         527       1.00      0.83      0.91         6\n",
      "         528       0.50      0.75      0.60         8\n",
      "         529       0.91      1.00      0.95        10\n",
      "         530       1.00      1.00      1.00         3\n",
      "         531       1.00      1.00      1.00         4\n",
      "         532       1.00      1.00      1.00         5\n",
      "         533       1.00      1.00      1.00         5\n",
      "         534       1.00      1.00      1.00         6\n",
      "         535       0.50      1.00      0.67         6\n",
      "         536       0.60      1.00      0.75         3\n",
      "         537       1.00      0.25      0.40         4\n",
      "         538       1.00      1.00      1.00         3\n",
      "         539       1.00      1.00      1.00         4\n",
      "         540       0.62      1.00      0.77         5\n",
      "         541       1.00      1.00      1.00         8\n",
      "         542       1.00      0.75      0.86         4\n",
      "         543       1.00      0.75      0.86         4\n",
      "         544       1.00      0.98      0.99       235\n",
      "         545       1.00      0.91      0.95        11\n",
      "         546       1.00      0.33      0.50         3\n",
      "         547       1.00      1.00      1.00         6\n",
      "         548       0.80      1.00      0.89         4\n",
      "         549       1.00      1.00      1.00         8\n",
      "         550       0.45      1.00      0.62         9\n",
      "         551       0.75      1.00      0.86         3\n",
      "         552       1.00      1.00      1.00         3\n",
      "         553       1.00      0.67      0.80         3\n",
      "         554       0.38      1.00      0.55        14\n",
      "         555       1.00      1.00      1.00         3\n",
      "         556       1.00      0.89      0.94         9\n",
      "         557       1.00      1.00      1.00         4\n",
      "         558       0.92      1.00      0.96        22\n",
      "         559       1.00      0.88      0.93         8\n",
      "         560       1.00      0.67      0.80        12\n",
      "         561       1.00      1.00      1.00         3\n",
      "         562       1.00      0.82      0.90        11\n",
      "         563       1.00      1.00      1.00         5\n",
      "         564       0.60      1.00      0.75         3\n",
      "         565       1.00      0.40      0.57         5\n",
      "         566       1.00      1.00      1.00        27\n",
      "         567       0.75      1.00      0.86         3\n",
      "         568       1.00      0.67      0.80         3\n",
      "         569       1.00      0.92      0.96        13\n",
      "         570       1.00      0.62      0.77         8\n",
      "         571       1.00      1.00      1.00         2\n",
      "         572       1.00      1.00      1.00         4\n",
      "         573       1.00      1.00      1.00         7\n",
      "         574       1.00      1.00      1.00        15\n",
      "         575       1.00      1.00      1.00         3\n",
      "         576       1.00      1.00      1.00         4\n",
      "         577       1.00      1.00      1.00         7\n",
      "         578       1.00      1.00      1.00         4\n",
      "         579       0.56      1.00      0.71         5\n",
      "         580       1.00      0.67      0.80         3\n",
      "         581       1.00      0.67      0.80         3\n",
      "         582       1.00      0.62      0.77         8\n",
      "         583       1.00      1.00      1.00         3\n",
      "         584       1.00      1.00      1.00         3\n",
      "         585       0.67      1.00      0.80         8\n",
      "         586       0.96      0.96      0.96        24\n",
      "         587       1.00      1.00      1.00         4\n",
      "         588       1.00      0.67      0.80         3\n",
      "         589       1.00      0.80      0.89         5\n",
      "         590       1.00      0.88      0.93         8\n",
      "         591       1.00      1.00      1.00         7\n",
      "         592       1.00      1.00      1.00         3\n",
      "         593       1.00      0.25      0.40         4\n",
      "         594       1.00      1.00      1.00        11\n",
      "         595       0.67      1.00      0.80        12\n",
      "         596       0.67      0.67      0.67         3\n",
      "         597       1.00      0.80      0.89         5\n",
      "         598       1.00      0.88      0.93         8\n",
      "         599       1.00      0.73      0.84        11\n",
      "         600       1.00      1.00      1.00         4\n",
      "         601       1.00      0.75      0.86         4\n",
      "         602       1.00      1.00      1.00         4\n",
      "         603       1.00      1.00      1.00         5\n",
      "         604       1.00      1.00      1.00         3\n",
      "         605       0.55      1.00      0.71        17\n",
      "         606       1.00      0.67      0.80         3\n",
      "         607       1.00      1.00      1.00         5\n",
      "         608       0.75      1.00      0.86         3\n",
      "         609       0.67      1.00      0.80         4\n",
      "         610       1.00      1.00      1.00         4\n",
      "         611       1.00      0.75      0.86         4\n",
      "         612       0.53      0.89      0.67         9\n",
      "         613       1.00      1.00      1.00         3\n",
      "         614       1.00      0.95      0.98        22\n",
      "         615       1.00      1.00      1.00         3\n",
      "         616       0.75      1.00      0.86         3\n",
      "         617       1.00      1.00      1.00         3\n",
      "         618       1.00      1.00      1.00         3\n",
      "         619       0.67      0.80      0.73         5\n",
      "         620       1.00      1.00      1.00         3\n",
      "         621       1.00      1.00      1.00         3\n",
      "         622       1.00      0.33      0.50         3\n",
      "         623       1.00      1.00      1.00         3\n",
      "         624       1.00      1.00      1.00         4\n",
      "         625       1.00      1.00      1.00         4\n",
      "         626       1.00      1.00      1.00         5\n",
      "         627       1.00      1.00      1.00         3\n",
      "         628       0.75      1.00      0.86         3\n",
      "         629       0.43      1.00      0.60         3\n",
      "         630       1.00      0.67      0.80         3\n",
      "         631       1.00      0.77      0.87        13\n",
      "         632       1.00      1.00      1.00         4\n",
      "         633       0.75      1.00      0.86         3\n",
      "         634       1.00      0.67      0.80         3\n",
      "         635       1.00      0.33      0.50         3\n",
      "         636       1.00      1.00      1.00        11\n",
      "         637       1.00      0.67      0.80         3\n",
      "         638       1.00      0.64      0.78        11\n",
      "         639       0.42      1.00      0.59         5\n",
      "         640       1.00      0.67      0.80         3\n",
      "         641       1.00      1.00      1.00         6\n",
      "         642       1.00      1.00      1.00         3\n",
      "         643       0.93      0.93      0.93        14\n",
      "         644       0.80      0.80      0.80         5\n",
      "         645       1.00      0.67      0.80         3\n",
      "         646       1.00      0.33      0.50         9\n",
      "         647       1.00      1.00      1.00         3\n",
      "         648       1.00      1.00      1.00         7\n",
      "         649       1.00      1.00      1.00         3\n",
      "         650       1.00      1.00      1.00         5\n",
      "         651       0.80      1.00      0.89         8\n",
      "         652       1.00      1.00      1.00         4\n",
      "         653       0.89      1.00      0.94         8\n",
      "         654       0.55      1.00      0.71        17\n",
      "         655       1.00      1.00      1.00         4\n",
      "         656       0.67      1.00      0.80         4\n",
      "         657       0.75      1.00      0.86         3\n",
      "         658       1.00      0.86      0.92        71\n",
      "         659       1.00      0.89      0.94        19\n",
      "         660       1.00      0.75      0.86         4\n",
      "         661       0.95      1.00      0.97        19\n",
      "         662       1.00      1.00      1.00         5\n",
      "         663       1.00      0.67      0.80         3\n",
      "         664       0.83      1.00      0.91         5\n",
      "         665       1.00      0.75      0.86        12\n",
      "         666       0.38      1.00      0.55         3\n",
      "         667       0.86      0.86      0.86         7\n",
      "         668       1.00      0.75      0.86         4\n",
      "         669       0.95      0.93      0.94        42\n",
      "         670       0.54      1.00      0.70         7\n",
      "         671       1.00      1.00      1.00         5\n",
      "         672       1.00      1.00      1.00         3\n",
      "         673       1.00      1.00      1.00         3\n",
      "         674       1.00      1.00      1.00         3\n",
      "         675       0.93      0.88      0.90        16\n",
      "         676       1.00      1.00      1.00         3\n",
      "         677       1.00      0.67      0.80         3\n",
      "         678       1.00      0.80      0.89        10\n",
      "         679       1.00      1.00      1.00         4\n",
      "         680       0.20      1.00      0.33         3\n",
      "         681       0.00      0.00      0.00         3\n",
      "         682       1.00      1.00      1.00         3\n",
      "         683       1.00      1.00      1.00         4\n",
      "         684       1.00      1.00      1.00         4\n",
      "         685       1.00      1.00      1.00         3\n",
      "         686       1.00      0.95      0.97        19\n",
      "         687       1.00      0.89      0.94         9\n",
      "         688       0.81      0.96      0.88        23\n",
      "         689       1.00      0.86      0.92         7\n",
      "         690       1.00      1.00      1.00         4\n",
      "         691       0.62      1.00      0.77         5\n",
      "         692       0.78      1.00      0.88         7\n",
      "         693       1.00      0.75      0.86         4\n",
      "         694       1.00      1.00      1.00         4\n",
      "         695       1.00      0.50      0.67         4\n",
      "         696       1.00      0.62      0.77         8\n",
      "         697       0.75      1.00      0.86         3\n",
      "         698       0.83      1.00      0.91         5\n",
      "         699       1.00      0.75      0.86         8\n",
      "         700       1.00      0.33      0.50         6\n",
      "         701       1.00      0.75      0.86         8\n",
      "         702       1.00      0.67      0.80         3\n",
      "         703       1.00      0.62      0.77         8\n",
      "         704       1.00      0.33      0.50         3\n",
      "         705       0.55      1.00      0.71         6\n",
      "         706       1.00      1.00      1.00         4\n",
      "         707       0.83      1.00      0.91         5\n",
      "         708       1.00      1.00      1.00        19\n",
      "         709       1.00      0.50      0.67         4\n",
      "         710       1.00      0.67      0.80         3\n",
      "         711       0.92      0.86      0.89        14\n",
      "         712       0.00      0.00      0.00         5\n",
      "         713       1.00      1.00      1.00         3\n",
      "         714       1.00      1.00      1.00         3\n",
      "         715       1.00      0.75      0.86         8\n",
      "         716       0.86      1.00      0.92         6\n",
      "         717       0.75      0.75      0.75         4\n",
      "         718       1.00      1.00      1.00         3\n",
      "         719       1.00      0.50      0.67         4\n",
      "         720       1.00      0.93      0.97        60\n",
      "         721       1.00      1.00      1.00         5\n",
      "         722       1.00      1.00      1.00         3\n",
      "         723       1.00      1.00      1.00         4\n",
      "         724       1.00      1.00      1.00         3\n",
      "         725       1.00      1.00      1.00         3\n",
      "         726       1.00      0.67      0.80         3\n",
      "         727       1.00      0.75      0.86         4\n",
      "         728       0.83      1.00      0.91         5\n",
      "         729       0.75      1.00      0.86         3\n",
      "         730       1.00      1.00      1.00         4\n",
      "         731       1.00      0.89      0.94         9\n",
      "         732       1.00      1.00      1.00         7\n",
      "         733       1.00      0.93      0.96        44\n",
      "         734       1.00      0.20      0.33         5\n",
      "         735       1.00      0.80      0.89         5\n",
      "         736       1.00      1.00      1.00         7\n",
      "         737       1.00      0.43      0.60         7\n",
      "         738       1.00      0.67      0.80         3\n",
      "         739       1.00      0.67      0.80         3\n",
      "         740       1.00      1.00      1.00        17\n",
      "         741       1.00      1.00      1.00         3\n",
      "         742       1.00      0.40      0.57         5\n",
      "         743       1.00      1.00      1.00         3\n",
      "         744       1.00      1.00      1.00         5\n",
      "         745       1.00      1.00      1.00         3\n",
      "         746       0.92      0.85      0.88        13\n",
      "         747       1.00      0.59      0.74        39\n",
      "         748       1.00      0.71      0.83         7\n",
      "         749       1.00      0.75      0.86         4\n",
      "         750       0.00      0.00      0.00         3\n",
      "         751       1.00      1.00      1.00         4\n",
      "         752       1.00      0.89      0.94         9\n",
      "         753       1.00      1.00      1.00        39\n",
      "         754       1.00      0.60      0.75         5\n",
      "         755       0.90      1.00      0.95         9\n",
      "         756       1.00      1.00      1.00        19\n",
      "         757       1.00      0.67      0.80         3\n",
      "         758       0.96      1.00      0.98        75\n",
      "         759       1.00      1.00      1.00         3\n",
      "         760       1.00      0.97      0.98        30\n",
      "         761       1.00      0.83      0.91         6\n",
      "         762       1.00      1.00      1.00         3\n",
      "         763       1.00      1.00      1.00         3\n",
      "         764       1.00      1.00      1.00        14\n",
      "         765       1.00      1.00      1.00        22\n",
      "         766       0.71      1.00      0.83         5\n",
      "         767       1.00      1.00      1.00         4\n",
      "         768       1.00      0.80      0.89        15\n",
      "         769       0.57      1.00      0.73         4\n",
      "         770       0.50      1.00      0.67         5\n",
      "         771       1.00      1.00      1.00         3\n",
      "         772       0.50      1.00      0.67         6\n",
      "         773       1.00      1.00      1.00         3\n",
      "         774       0.83      1.00      0.91         5\n",
      "         775       0.00      0.00      0.00         3\n",
      "         776       0.94      0.97      0.95        32\n",
      "         777       1.00      0.80      0.89         5\n",
      "         778       1.00      0.67      0.80         3\n",
      "         779       0.87      1.00      0.93        13\n",
      "         780       0.99      0.98      0.99       528\n",
      "         781       0.75      0.50      0.60         6\n",
      "         782       1.00      1.00      1.00         9\n",
      "         783       1.00      0.33      0.50         3\n",
      "         784       1.00      0.67      0.80         3\n",
      "         785       0.79      1.00      0.88        33\n",
      "         786       1.00      1.00      1.00         6\n",
      "         787       1.00      0.73      0.85        15\n",
      "         788       1.00      0.90      0.95        20\n",
      "         789       1.00      1.00      1.00         3\n",
      "         790       1.00      1.00      1.00         3\n",
      "         791       1.00      0.80      0.89         5\n",
      "         792       1.00      1.00      1.00         4\n",
      "         793       0.46      1.00      0.63        12\n",
      "         794       1.00      0.89      0.94         9\n",
      "         795       1.00      1.00      1.00         3\n",
      "         796       1.00      1.00      1.00        24\n",
      "         797       0.82      1.00      0.90         9\n",
      "         798       1.00      1.00      1.00         4\n",
      "         799       1.00      1.00      1.00         4\n",
      "         800       1.00      0.83      0.91         6\n",
      "         801       1.00      0.91      0.95        11\n",
      "         802       0.60      1.00      0.75         3\n",
      "         803       1.00      0.80      0.89         5\n",
      "         804       1.00      0.96      0.98        52\n",
      "         805       1.00      1.00      1.00         3\n",
      "         806       1.00      0.67      0.80         3\n",
      "         807       0.90      0.64      0.75        14\n",
      "         808       1.00      1.00      1.00         3\n",
      "         809       1.00      1.00      1.00         7\n",
      "         810       1.00      0.50      0.67         4\n",
      "         811       1.00      0.80      0.89         5\n",
      "         812       1.00      1.00      1.00         3\n",
      "         813       1.00      1.00      1.00         7\n",
      "         814       1.00      1.00      1.00         6\n",
      "         815       1.00      1.00      1.00         3\n",
      "         816       1.00      1.00      1.00         4\n",
      "         817       0.80      1.00      0.89         4\n",
      "         818       0.50      1.00      0.67         3\n",
      "         819       1.00      1.00      1.00         4\n",
      "         820       1.00      0.50      0.67         4\n",
      "         821       1.00      0.97      0.98        33\n",
      "         822       1.00      0.80      0.89         5\n",
      "         823       0.75      0.75      0.75         4\n",
      "         824       1.00      1.00      1.00         3\n",
      "         825       1.00      1.00      1.00         3\n",
      "         826       1.00      1.00      1.00        17\n",
      "         827       1.00      0.75      0.86         4\n",
      "         828       0.80      0.67      0.73         6\n",
      "         829       1.00      0.75      0.86         4\n",
      "         830       1.00      0.50      0.67         2\n",
      "         831       0.00      0.00      0.00         3\n",
      "         832       1.00      1.00      1.00         4\n",
      "         833       1.00      1.00      1.00         6\n",
      "         834       1.00      1.00      1.00         4\n",
      "         835       1.00      1.00      1.00         3\n",
      "         836       1.00      1.00      1.00         3\n",
      "         837       1.00      1.00      1.00         4\n",
      "         838       1.00      1.00      1.00         3\n",
      "         839       1.00      1.00      1.00         5\n",
      "         840       0.62      1.00      0.77         5\n",
      "         841       1.00      0.25      0.40         4\n",
      "         842       0.95      0.76      0.84        25\n",
      "         843       1.00      0.67      0.80         3\n",
      "         844       1.00      0.80      0.89         5\n",
      "         845       1.00      1.00      1.00         5\n",
      "         846       0.40      0.67      0.50         3\n",
      "         847       0.67      0.40      0.50         5\n",
      "         848       1.00      0.75      0.86         4\n",
      "         849       1.00      1.00      1.00         3\n",
      "         850       1.00      0.75      0.86         4\n",
      "         851       0.83      0.83      0.83         6\n",
      "         852       1.00      1.00      1.00        13\n",
      "         853       0.83      1.00      0.91        30\n",
      "         854       1.00      0.80      0.89         5\n",
      "         855       1.00      1.00      1.00         3\n",
      "         856       0.36      1.00      0.53         4\n",
      "         857       0.56      1.00      0.71         5\n",
      "         858       1.00      0.83      0.91         6\n",
      "         859       1.00      0.75      0.86         4\n",
      "         860       0.56      1.00      0.71         5\n",
      "         861       1.00      1.00      1.00         5\n",
      "         862       1.00      1.00      1.00         3\n",
      "         863       1.00      0.25      0.40         4\n",
      "         864       1.00      1.00      1.00         4\n",
      "         865       1.00      1.00      1.00        14\n",
      "         866       1.00      0.67      0.80         3\n",
      "         867       1.00      1.00      1.00         3\n",
      "         868       1.00      1.00      1.00         4\n",
      "         869       1.00      1.00      1.00         3\n",
      "         870       0.78      1.00      0.88        18\n",
      "         871       1.00      1.00      1.00         9\n",
      "         872       1.00      1.00      1.00         4\n",
      "         873       1.00      1.00      1.00         3\n",
      "         874       1.00      0.82      0.90        11\n",
      "         875       0.80      1.00      0.89         4\n",
      "         876       1.00      0.67      0.80         3\n",
      "         877       1.00      0.75      0.86         4\n",
      "         878       1.00      1.00      1.00         3\n",
      "         879       1.00      0.60      0.75         5\n",
      "         880       0.76      1.00      0.86        16\n",
      "         881       1.00      1.00      1.00         4\n",
      "         882       1.00      1.00      1.00         3\n",
      "         883       0.32      1.00      0.48         6\n",
      "         884       1.00      1.00      1.00         6\n",
      "         885       0.85      1.00      0.92        41\n",
      "         886       1.00      0.88      0.93         8\n",
      "         887       1.00      1.00      1.00         7\n",
      "         888       1.00      0.96      0.98        53\n",
      "         889       1.00      1.00      1.00         4\n",
      "         890       1.00      0.80      0.89         5\n",
      "         891       1.00      1.00      1.00         3\n",
      "         892       1.00      1.00      1.00        15\n",
      "         893       1.00      1.00      1.00         3\n",
      "         894       1.00      0.67      0.80         3\n",
      "         895       1.00      1.00      1.00         3\n",
      "         896       1.00      0.93      0.96        14\n",
      "         897       1.00      1.00      1.00         7\n",
      "         898       0.67      1.00      0.80         4\n",
      "         899       1.00      0.80      0.89         5\n",
      "         900       0.67      1.00      0.80         8\n",
      "\n",
      "    accuracy                           0.91      7551\n",
      "   macro avg       0.92      0.88      0.88      7551\n",
      "weighted avg       0.94      0.91      0.91      7551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"+++++++++++Classification Report++++++++++++++++++++++++\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "producttemplate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
