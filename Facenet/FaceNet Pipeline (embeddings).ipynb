{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbd0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\python.exe\n"
     ]
    }
   ],
   "source": [
    "'''Importing the dependencies and libraries'''\n",
    "import sys, dlib, random, os, time\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Analytics, ML and Visualization libraries\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as img_plt \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from utils.download import download_url_to_file\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c988b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ac641",
   "metadata": {},
   "source": [
    "1. Provide the required values for the variables (face_detector, img_path, data_fol, im_size) that are assinged!\n",
    "2. Provide the classes folder name in numeric values like (0,1,2,...)\n",
    "3. Provide the faces images in respected classes folder.\n",
    "Data Folder Structure:\n",
    "        --> data:\n",
    "               --> class 1:\n",
    "                       --> Image 1\n",
    "                       --> Image 2\n",
    "                       --> Image 3\n",
    "                           .\n",
    "                           .\n",
    "                           .\n",
    "               --> class 2:\n",
    "                       --> Image 1\n",
    "                       --> Image 2\n",
    "                       --> Image 3\n",
    "                           .\n",
    "                           .\n",
    "                           ....\n",
    "\n",
    "                   .\n",
    "                   .\n",
    "                   ....\n",
    "4. Provide face detector 'dlib' or None. Default cv2 cascade classifier is selected.\n",
    "5. Provide the  im_size depends of model input shape [224*224] is selected currently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62eaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigin \"dlib\" to run dib face detector\n",
    "# Assign \"None\" to run cv2 Cascade Classifier \n",
    "face_detector=\"dlib\" \n",
    "img_path= r\"path\\0\\Claudia_Pechstein_0003.jpg\" # image path \n",
    "data_fol=r\"path\" # provide the data directory path\n",
    "im_size= 224#image size\n",
    "\n",
    "# Default 'cv2.CascadeClassifier' is selected \n",
    "if face_detector==\"dlib\":\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "else:    \n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91357f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select True if you want to view the image\n",
    "if False:\n",
    "    img = cv2.imread(img_path) \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    if face_detector==\"dlib\":\n",
    "        faces = detector(gray)\n",
    "        print(faces)\n",
    "        for face in faces:\n",
    "            x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2) \n",
    "    else:\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "        for (x,y,w,h) in faces: \n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2) \n",
    "            roi_gray = gray[y:y+h, x:x+w] \n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "                \n",
    "    cv2.imshow('img',img) \n",
    "    cv2.waitKey(10)\n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b1053",
   "metadata": {},
   "source": [
    "To prepare \"data.csv\" datafame with columns \"Image Path\", \"Face\", \"Bounding Box\", \"Aspect ratio\", \"Target\" for training purpose with pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b051983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes=os.listdir(data_fol)\n",
    "rows_list=[]\n",
    "for i in tqdm(classes):\n",
    "    fol=data_fol+f\"\\{i}\"\n",
    "    imgs=os.listdir(fol)\n",
    "    for j in imgs:\n",
    "        img = cv2.imread(fol+f\"\\{j}\") \n",
    "        img=cv2.resize(img, (im_size, im_size), interpolation = cv2.INTER_LINEAR)\n",
    "        H, W, c = img.shape\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "        if face_detector==\"dlib\":\n",
    "            faces = detector(gray)\n",
    "            cls_n=None\n",
    "            if faces:\n",
    "                face=faces[0]\n",
    "                x, y, w, h = (face.left(), face.top(), face.width(), face.height());bb=abs(x),abs(y),abs(x+w),abs(y+h)\n",
    "            else:\n",
    "                bb= (0,0,W,H)\n",
    "                cls_n=len(classes)+1   \n",
    "            rows_list.append({'Image Path':fol+f\"\\{j}\", 'Face': 1 if faces else 0, 'Bounding Box':bb ,'Aspect ratio':(w,h),'Target':cls_n if cls_n else i})\n",
    "        else:\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "            cls_n=None\n",
    "            if type(faces)!=tuple and faces.any():\n",
    "                x,y,w,h=faces[0];bb=abs(x),abs(y),abs(x+w),abs(y+h)\n",
    "            else:\n",
    "                bb= (0,0,W,H)\n",
    "                cls_n=len(classes)+1\n",
    "            rows_list.append({'Image Path':fol+f\"\\{j}\", 'Face': 1 if type(faces)!=tuple and faces.any() else 0, 'Bounding Box':bb ,'Aspect ratio':(w,h),'Target':cls_n if cls_n else i})\n",
    "df = pd.DataFrame(rows_list) \n",
    "df.astype(object)\n",
    "display(df.head())   \n",
    "df.to_csv(\"data.csv\",index=False)\n",
    "print(\"================= 'data.csv' Generated =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b42a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7606 entries, 0 to 7605\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Image Path    7606 non-null   object\n",
      " 1   Face          7606 non-null   int64 \n",
      " 2   Bounding Box  7606 non-null   object\n",
      " 3   Aspect ratio  7606 non-null   object\n",
      " 4   Target        7606 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 297.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # provide the details of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6fc1cb",
   "metadata": {},
   "source": [
    "kmeans cluster and k nearest neighbor algorithim is used to find anchor and to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cd1432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clustering Algorithm\n",
    "def kmeans_anchors(boxes, k):\n",
    "    # Normalize boxes\n",
    "    boxes = np.array(boxes)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(boxes)\n",
    "    anchors = kmeans.cluster_centers_\n",
    "    return anchors\n",
    "\n",
    "face_df=df[df[\"Face\"]==1]\n",
    "print(face_df.shape)\n",
    "anchor_=kmeans_anchors(face_df[\"Aspect ratio\"].tolist(),1)\n",
    "anchor_=np.array(anchor_[0]).astype(int)\n",
    "face_df[\"Anchor\"]=[list(anchor_)]*face_df.shape[0]\n",
    "\n",
    "''' To find cluster with respective to classes'''\n",
    "# anchor_dic={}\n",
    "# for un_ in face_df[\"Target\"].unique().tolist():\n",
    "#     tempdf=face_df[face_df[\"Target\"]==un_]\n",
    "#     anchor_dic.update({un_:kmeans_anchors(tempdf[\"Aspect ratio\"].tolist(), 1)[0]})\n",
    "# print(anchor_dic)  \n",
    "\n",
    "display(face_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74acbee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchor_im={} #anchor image\n",
    "anchor_bb={} #anchor bounding box\n",
    "\n",
    "for un_ in tqdm(face_df[\"Target\"].unique().tolist()):\n",
    "    tempdf=face_df[face_df[\"Target\"]==un_]\n",
    "    points=[tuple(tempdf[\"Anchor\"].tolist()[0])]+tempdf[\"Aspect ratio\"].tolist()\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(points)\n",
    "    distances, indices = nbrs.kneighbors(points)\n",
    "    if indices.any():\n",
    "        anchor_im.update({un_:tempdf.iloc[indices[0][1]-1][\"Image Path\"]})\n",
    "        anchor_bb.update({un_:tempdf.iloc[indices[0][1]-1][\"Bounding Box\"]})\n",
    "\n",
    "def anchor_image(row):\n",
    "    global anchor_im\n",
    "    return anchor_im[row[\"Target\"]]\n",
    "\n",
    "def anchor_boundingbox(row):\n",
    "    global anchor_bb\n",
    "    return anchor_bb[row[\"Target\"]]    \n",
    "    \n",
    "    \n",
    "\n",
    "face_df['Anchor Image'] = face_df.apply(anchor_image, axis=1)\n",
    "face_df['Anchor bb'] = face_df.apply(anchor_boundingbox, axis=1)\n",
    "\n",
    "display(face_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02fc21",
   "metadata": {},
   "source": [
    "To prepare final \"face_data.csv\" datafame with columns \"Image Path\", \"Face\", \"Bounding Box\", \"Aspect ratio\", \"Target\", \"Anchor\", \"Anchor Image\", \"Anchor bb\", \"Negative Image\", \"Negative bb\" for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c662df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_ls=[] #negative image path\n",
    "neg_bb=[] #negative face bounding box\n",
    "\n",
    "for row in tqdm(face_df.iterrows()):\n",
    "    ex_id=row[1][\"Target\"]\n",
    "    tempdf=face_df[face_df[\"Target\"]!=ex_id]\n",
    "    rand_path=random.choice(tempdf[\"Image Path\"].tolist())\n",
    "    rand_bb=tempdf[tempdf[\"Image Path\"]==rand_path][\"Bounding Box\"].tolist()[0]\n",
    "    neg_ls.append(rand_path)\n",
    "    neg_bb.append(rand_bb)\n",
    "\n",
    "face_df[\"Negative Image\"]=neg_ls\n",
    "face_df[\"Negative bb\"]=neg_bb\n",
    "display(face_df.head())\n",
    "face_df.to_csv(\"face_data.csv\",index=False)\n",
    "print(\"++++++++++++++++++++++++ 'face_data.csv' is Generated ++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26118d38",
   "metadata": {},
   "source": [
    "Visualising and analysing the non detected face images by face detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d422b457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df[df[\"Face\"]==0]); len(df[df[\"Face\"]==0]) # Filtering the non-detected face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403a7a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising  the non- detected face and anlysis\n",
    "for row in df[df[\"Face\"]==0].iterrows():\n",
    "    print(row[1][\"Image Path\"])\n",
    "    im = img_plt.imread(row[1][\"Image Path\"]) \n",
    "    plt.imshow(im) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ca1dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of face detected by dlib face detector in provided dataset 99.35577175913753\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dlib face detector detected around 99% of face with provided dataset\n",
    "cv2 cascade classifier detected around 95% of face with provided dataset\n",
    "'''\n",
    "\n",
    "print(\"Percentage of face detected by dlib face detector in provided dataset\",(df[\"Face\"].sum()/df.shape[0])*100) # cv2 face cascade 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa170e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af553e71",
   "metadata": {},
   "source": [
    "# FaceNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663a723",
   "metadata": {},
   "source": [
    "FaceNet is the name of the facial recognition system that was proposed by Google Researchers in 2015 in the paper titled FaceNet: A Unified Embedding for Face Recognition and Clustering.\n",
    "\n",
    "Reference:\n",
    "\n",
    "    -> https://github.com/davidsandberg/facenet\n",
    "    -> https://github.com/timesler/facenet-pytorch/tree/master\n",
    "    -> https://medium.com/@mohitsaini_54300/train-facenet-with-triplet-loss-for-real-time-face-recognition-a39e2f4472c3\n",
    "    -> https://datahacker.rs/025-facenet-a-unified-embedding-for-face-recognition-and-clustering-in-pytorch/\n",
    "    -> https://www.geeksforgeeks.org/facenet-using-facial-recognition-system/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da00f78",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "            Input images [anchor, positive, negative]\n",
    "            Batch Input [Tensor:torch.Size([1, 3, Image size, Image size])]\n",
    "                               ⬇️\n",
    "            FaceNet DNN network (InceptionResnetV1 model)\n",
    "                    (pretrained vggface weights)\n",
    "                               ⬇️\n",
    "                face embbedings [torch.Size([1, 512])]\n",
    "                               ⬇️\n",
    "                      Triplet loss function                      \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7279d",
   "metadata": {},
   "source": [
    "This is a Pytorch implementation of the face recognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d57455",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52708f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872dfba11564439e9cca47369d2968c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224]) torch.Size([8, 3, 224, 224]) torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "imageSize=224 #input image size\n",
    "\n",
    "'''\n",
    "Custom dataloader for Facenet model.\n",
    "Returns: anchor (Tensor:torch.Size([1, 3, 224, 224])), positive (Tensor:torch.Size([1, 3, 224, 224])), negative (Tensor:torch.Size([1, 3, 224, 224]))\n",
    "'''\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, csv_path, imageSize,transform=None):\n",
    "        self.df_data=pd.read_csv(csv_path)  \n",
    "        self.transform = transform\n",
    "        self.positiveImages = self.df_data[\"Image Path\"].tolist()\n",
    "        self.negativeImages = self.df_data[\"Negative Image\"].tolist()\n",
    "        self.anchorImages = self.df_data[\"Anchor Image\"].tolist()\n",
    "        self.pos_bb = self.df_data[\"Bounding Box\"].tolist()\n",
    "        self.neg_bb=self.df_data[\"Negative bb\"].tolist()\n",
    "        self.anchor_bb=self.df_data[\"Anchor bb\"].tolist()\n",
    "        self.im_size=imageSize\n",
    "    \n",
    "    \n",
    "    def read_image(self,path,bb):\n",
    "        try:\n",
    "            image=cv2.imread(path)\n",
    "            image=cv2.resize(image, (self.im_size, self.im_size), interpolation = cv2.INTER_LINEAR)\n",
    "            image=image[bb[1]:bb[3],bb[0]:bb[2]]\n",
    "            image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image=Image.fromarray(image)\n",
    "            return image \n",
    "        except Exception as e:\n",
    "            print(\"============================================\")\n",
    "            print(e,path,bb)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.positiveImages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos = self.read_image(self.positiveImages[idx],eval(self.pos_bb[idx]))\n",
    "        neg= self.read_image(self.negativeImages[idx],eval(self.neg_bb[idx]))\n",
    "        anchor= self.read_image(self.anchorImages[idx],eval(self.anchor_bb[idx]))\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            positive = self.transform(pos)\n",
    "            negative = self.transform(neg)\n",
    "            anchor = self.transform(anchor)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "# Define transformations for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imageSize, imageSize)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = customDataset('./face_data.csv', imageSize, transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for anchor, positive, negative in tqdm(dataloader):\n",
    "    print(anchor.size(), positive.size(),negative.size()) #size of the tensor\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664ca20",
   "metadata": {},
   "source": [
    "# Model arhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc99f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_home():\n",
    "    torch_home = os.path.expanduser(\n",
    "        os.getenv(\n",
    "            'TORCH_HOME',\n",
    "            os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')\n",
    "        )\n",
    "    )\n",
    "    return torch_home\n",
    "\n",
    "def load_weights(mdl):\n",
    "    path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt' #'vggface2' weights\n",
    "    \n",
    "    model_dir = os.path.join(get_torch_home(), 'checkpoints')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    cached_file = os.path.join(model_dir, os.path.basename(path))\n",
    "    if not os.path.exists(cached_file):\n",
    "        download_url_to_file(path, cached_file)\n",
    "\n",
    "    state_dict = torch.load(cached_file)\n",
    "    mdl.load_state_dict(state_dict)\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_planes, out_planes,\n",
    "            kernel_size=kernel_size, stride=stride,\n",
    "            padding=padding, bias=False\n",
    "        ) # verify bias false\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_planes,\n",
    "            eps=0.001, # value found in tensorflow\n",
    "            momentum=0.1, # default pytorch value\n",
    "            affine=True\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block35(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block17(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 128, kernel_size=1, stride=1),\n",
    "            BasicConv2d(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(128, 128, kernel_size=(7,1), stride=1, padding=(3,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block8(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0, noReLU=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.noReLU = noReLU\n",
    "\n",
    "        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(1792, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
    "            BasicConv2d(192, 192, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\n",
    "        if not self.noReLU:\n",
    "            self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        if not self.noReLU:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_6a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_7a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InceptionResnetV1(nn.Module):\n",
    "    def __init__(self, pretrained=\"vggface2\", classify=False, num_classes=None, dropout_prob=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.classify = classify\n",
    "        self.num_classes = num_classes\n",
    "        tmp_classes = 8631  #'vggface2' output layer\n",
    "\n",
    "        # Define layers\n",
    "        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n",
    "        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n",
    "        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        self.repeat_1 = nn.Sequential(\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "        )\n",
    "        self.mixed_6a = Mixed_6a()\n",
    "        self.repeat_2 = nn.Sequential(\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "        )\n",
    "        self.mixed_7a = Mixed_7a()\n",
    "        self.repeat_3 = nn.Sequential(\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "        )\n",
    "        self.block8 = Block8(noReLU=True)\n",
    "        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.last_linear = nn.Linear(1792, 512, bias=False)\n",
    "        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.logits = nn.Linear(512, tmp_classes)\n",
    "            load_weights(self)\n",
    "\n",
    "        if self.classify and self.num_classes is not None:\n",
    "            self.logits = nn.Linear(512, self.num_classes)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(device)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate embeddings or logits given a batch of input image tensors.\n",
    "\n",
    "        Arguments:\n",
    "            x {torch.tensor} -- Batch of image tensors representing faces.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor -- Batch of embedding vectors or multinomial logits.\n",
    "        \"\"\"\n",
    "        x = self.conv2d_1a(x)\n",
    "        x = self.conv2d_2a(x)\n",
    "        x = self.conv2d_2b(x)\n",
    "        x = self.maxpool_3a(x)\n",
    "        x = self.conv2d_3b(x)\n",
    "        x = self.conv2d_4a(x)\n",
    "        x = self.conv2d_4b(x)\n",
    "        x = self.repeat_1(x)\n",
    "        x = self.mixed_6a(x)\n",
    "        x = self.repeat_2(x)\n",
    "        x = self.mixed_7a(x)\n",
    "        x = self.repeat_3(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.avgpool_1a(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x.view(x.shape[0], -1))\n",
    "        x = self.last_bn(x)\n",
    "        if self.classify:\n",
    "            x = self.logits(x)\n",
    "        else:\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329927e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29575ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving checkpoint for every epochs\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    \n",
    "# loading checkpoint \n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d771a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Epochs\n",
    "num_epochs = 100 #number of epochs\n",
    "\n",
    "checkpoint_fol = './model/' # provide directory of model to be saved\n",
    "checkpoint_path=None # provide the path to load the trained model\n",
    "\n",
    "model = InceptionResnetV1() #intialze the model\n",
    "triplet_loss = TripletLoss(margin=1.0) #intialze the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) #adam optimizer\n",
    "start_epoch=0\n",
    "# If 'checkpoint_path' not None and path is provided. The model will be loaded and weights will be intialized with trained weights\n",
    "if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "    model, optimizer, start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32dde7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484c7342f27249fe97a705eca19baadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb171a7cf504706a5eb31539ba5e048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4206757a06b544daa191b5e4729eaa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.8046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659654953b744c38839ea8d6330f9df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.7241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09084740b6864aaba722189becf61009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.6720\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d345d487a4476aac7f0a415d810c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.6496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbb3b6712734868b2d471da2f13f19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.5687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bddb2b31df49838cf1966cedd694e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.5241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042d50fb5b8345948fefd10c97409f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.4777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7279d03af44ba1939b4d86f5a56ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.4543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bb736e9dae4925a842750f9ae74bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af77ef3755564cb288ee2c88787bd6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m anchor_embedding \u001b[38;5;241m=\u001b[39m model(anchor)\n\u001b[1;32m---> 11\u001b[0m positive_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m negative_embedding \u001b[38;5;241m=\u001b[39m model(negative)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 275\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_6a(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_7a(x)\n\u001b[0;32m    277\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 101\u001b[0m, in \u001b[0;36mBlock17.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    100\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch0(x)\n\u001b[1;32m--> 101\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbranch1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x0, x1), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    103\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[0;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(start_epoch,num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    try:\n",
    "        for batch_idx, (anchor, positive, negative) in tqdm(enumerate(dataloader)):\n",
    "            anchor, positive, negative = anchor, positive, negative\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            anchor_embedding = model(anchor)\n",
    "            positive_embedding = model(positive)\n",
    "            negative_embedding = model(negative)\n",
    "            \n",
    "            loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            running_loss += loss.item()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "    file_=f'{epoch+1}_Loss_{running_loss/len(dataloader):.4f}'\n",
    "    save_checkpoint(model, optimizer, epoch, running_loss, checkpoint_fol+f\"{file_}.pt\")\n",
    "print(\"Finished Training\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37a3d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8900\n"
     ]
    }
   ],
   "source": [
    "'''To save the checkpoint for trained weights'''\n",
    "# print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "# file_=f'{epoch+1}_Loss_{running_loss/len(dataloader):.4f}'\n",
    "# save_checkpoint(model, optimizer, epoch, running_loss, checkpoint_fol+f\"{file_}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f909d4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3fc26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_recognition():\n",
    "    im_size=224\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((im_size, im_size)),  # Resize the image to the desired size\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "    master_embedding={}\n",
    "    \n",
    "    def __init__(self,model, optimizer,trained_data_df,checkpoint=None):\n",
    "        master_df=pd.read_csv(trained_data_df,usecols=[\"Anchor Image\",\"Anchor bb\",\"Target\"])\n",
    "        master_df=master_df.groupby(\"Anchor Image\").first()\n",
    "        self.model=model\n",
    "        if checkpoint:self.model=self.load_checkpoint(self.model, optimizer, checkpoint)\n",
    "        self.model.eval()    \n",
    "        for i in tqdm(master_df.iterrows()):\n",
    "            detect_img=self.read_image(i[0],eval(i[1][\"Anchor bb\"]))\n",
    "            with torch.no_grad():\n",
    "                face_embedding = self.model(detect_img.unsqueeze(0))\n",
    "            Face_recognition.master_embedding.update({i[1][\"Target\"]:face_embedding})    \n",
    "\n",
    "    def load_checkpoint(self,model, optimizer, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(\"loaded\")\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def read_image(self,path,bb):\n",
    "        image=cv2.imread(path)\n",
    "        image=cv2.resize(image, (self.im_size, self.im_size), interpolation = cv2.INTER_LINEAR)\n",
    "        image=image[bb[1]:bb[3],bb[0]:bb[2]]\n",
    "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image=Image.fromarray(image)\n",
    "        return Face_recognition.transform(image)    \n",
    "\n",
    "    def preprocess(self,image):\n",
    "        im_size= Face_recognition.im_size\n",
    "        image=cv2.resize(image, (im_size, im_size), interpolation = cv2.INTER_LINEAR)\n",
    "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image=Image.fromarray(image)\n",
    "        return image \n",
    "\n",
    "    def compare_embeddings(self,embedding1, embedding2, similarity_=False,threshold=0.6):\n",
    "        if similarity_:\n",
    "            cos = nn.CosineSimilarity(dim=1)\n",
    "            output = cos(embedding1, embedding2)\n",
    "            return float(output), bool(output> threshold)  \n",
    "        else:\n",
    "            distance = np.linalg.norm(embedding1.detach().numpy() - embedding2.detach().numpy())\n",
    "            return distance, distance < threshold\n",
    "    \n",
    "    def detect(self,img, similarity_=False,thresh=0.6):\n",
    "        detect_img=self.preprocess(img)\n",
    "        detect_img = Face_recognition.transform(detect_img)\n",
    "        with torch.no_grad():\n",
    "            face_embedding = self.model(detect_img.unsqueeze(0))\n",
    "\n",
    "        predictions=[]\n",
    "        for class_name, fea in Face_recognition.master_embedding.items():\n",
    "            val, flag =self.compare_embeddings(face_embedding, fea, similarity_,thresh)\n",
    "            predictions.append([class_name,val, flag])\n",
    "\n",
    "        predictions=np.array(predictions)\n",
    "        if predictions.any():\n",
    "            predictions=predictions[np.where(predictions[:,2]==True)]\n",
    "            if predictions.any():\n",
    "                if similarity_:\n",
    "                    predicted_class=predictions[predictions[:, 1].argsort()][-1][0]\n",
    "                    val=predictions[predictions[:, 1].argsort()][-1][1]\n",
    "                else:\n",
    "                    predicted_class=predictions[predictions[:, 1].argsort()][0][0]\n",
    "                    val=predictions[predictions[:, 1].argsort()][0][1] \n",
    "                return {predicted_class:val}, predictions[predictions[:, 1].argsort()]\n",
    "        return {}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa2d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6969f79bd174baea0e49c60a5b00a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path=r\"path\\6\\Mark_Richt_0002.jpg\"\n",
    "face_detector=\"dlib\"\n",
    "im_size=224 #image size\n",
    "check_point_path=\"./model 2/8_Loss_0.4777.pt\"\n",
    "master_data=\"./face_data.csv\"\n",
    "\n",
    "model = InceptionResnetV1()\n",
    "opt=optim.Adam(model.parameters(), lr=0.001)\n",
    "fr=Face_recognition(model,opt,master_data,check_point_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846828ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.4645073413848877\n",
      "Predition:  {317.0: 0.9998830556869507}\n"
     ]
    }
   ],
   "source": [
    "if face_detector==\"dlib\":\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "else:    \n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "    \n",
    "if True:\n",
    "    \n",
    "    start_time=time.time()\n",
    "    img = cv2.imread(img_path) \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    crop_img=[]\n",
    "    if face_detector==\"dlib\":\n",
    "        faces = detector(gray)\n",
    "        for face in faces:\n",
    "            x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "            # cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2)\n",
    "            crop_img.append(img[y:y+h,x:x+w])\n",
    "    else:\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "        for (x,y,w,h) in faces: \n",
    "            # cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2) \n",
    "            roi_gray = gray[y:y+h, x:x+w] \n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "            crop_img.append(roi_color)\n",
    "\n",
    "    results={}\n",
    "    \n",
    "    if crop_img:\n",
    "        for c_img in crop_img:\n",
    "            result,__=fr.detect(c_img, 0.6)\n",
    "            results.update(result)\n",
    "            \n",
    "    img=cv2.resize(img, (420,640), interpolation = cv2.INTER_LINEAR)    \n",
    "    img = cv2.putText(img, f\"Detected class: {results.keys() if results else 'Nil' }\", (5, 10), cv2.FONT_HERSHEY_SIMPLEX,  0.5, (255, 0, 0) , 1, cv2.LINE_AA)       \n",
    "    print(\"Time taken: \",time.time()-start_time) \n",
    "    print(\"Predition: \", results)\n",
    "    cv2.imshow('img',img) \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4ed8d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[362.    ,   0.6016,   1.    ],\n",
       "       [341.    ,   0.6022,   1.    ],\n",
       "       [307.    ,   0.6076,   1.    ],\n",
       "       [841.    ,   0.6085,   1.    ],\n",
       "       [452.    ,   0.6092,   1.    ],\n",
       "       [592.    ,   0.6112,   1.    ],\n",
       "       [832.    ,   0.6119,   1.    ],\n",
       "       [626.    ,   0.6159,   1.    ],\n",
       "       [474.    ,   0.6161,   1.    ],\n",
       "       [194.    ,   0.62  ,   1.    ],\n",
       "       [481.    ,   0.6216,   1.    ],\n",
       "       [760.    ,   0.6223,   1.    ],\n",
       "       [378.    ,   0.6268,   1.    ],\n",
       "       [601.    ,   0.6312,   1.    ],\n",
       "       [805.    ,   0.6315,   1.    ],\n",
       "       [723.    ,   0.632 ,   1.    ],\n",
       "       [479.    ,   0.6323,   1.    ],\n",
       "       [614.    ,   0.6328,   1.    ],\n",
       "       [375.    ,   0.635 ,   1.    ],\n",
       "       [874.    ,   0.6373,   1.    ],\n",
       "       [541.    ,   0.6384,   1.    ],\n",
       "       [300.    ,   0.6387,   1.    ],\n",
       "       [696.    ,   0.642 ,   1.    ],\n",
       "       [241.    ,   0.648 ,   1.    ],\n",
       "       [488.    ,   0.6487,   1.    ],\n",
       "       [900.    ,   0.649 ,   1.    ],\n",
       "       [284.    ,   0.6493,   1.    ],\n",
       "       [728.    ,   0.6501,   1.    ],\n",
       "       [775.    ,   0.6526,   1.    ],\n",
       "       [669.    ,   0.6605,   1.    ],\n",
       "       [275.    ,   0.6629,   1.    ],\n",
       "       [174.    ,   0.6634,   1.    ],\n",
       "       [ 75.    ,   0.6671,   1.    ],\n",
       "       [ 37.    ,   0.6693,   1.    ],\n",
       "       [232.    ,   0.6714,   1.    ],\n",
       "       [837.    ,   0.6732,   1.    ],\n",
       "       [884.    ,   0.6732,   1.    ],\n",
       "       [697.    ,   0.6822,   1.    ],\n",
       "       [485.    ,   0.6832,   1.    ],\n",
       "       [768.    ,   0.6917,   1.    ],\n",
       "       [848.    ,   0.6928,   1.    ],\n",
       "       [113.    ,   0.6935,   1.    ],\n",
       "       [477.    ,   0.6966,   1.    ],\n",
       "       [ 35.    ,   0.7004,   1.    ],\n",
       "       [888.    ,   0.7017,   1.    ],\n",
       "       [ 99.    ,   0.7038,   1.    ],\n",
       "       [857.    ,   0.7039,   1.    ],\n",
       "       [736.    ,   0.7053,   1.    ],\n",
       "       [731.    ,   0.7053,   1.    ],\n",
       "       [317.    ,   0.7088,   1.    ],\n",
       "       [649.    ,   0.7099,   1.    ],\n",
       "       [542.    ,   0.7099,   1.    ],\n",
       "       [834.    ,   0.7117,   1.    ],\n",
       "       [225.    ,   0.7129,   1.    ],\n",
       "       [719.    ,   0.7139,   1.    ],\n",
       "       [122.    ,   0.717 ,   1.    ],\n",
       "       [496.    ,   0.7193,   1.    ],\n",
       "       [172.    ,   0.7193,   1.    ],\n",
       "       [408.    ,   0.72  ,   1.    ],\n",
       "       [377.    ,   0.725 ,   1.    ],\n",
       "       [239.    ,   0.7279,   1.    ],\n",
       "       [179.    ,   0.7291,   1.    ],\n",
       "       [635.    ,   0.73  ,   1.    ],\n",
       "       [661.    ,   0.7362,   1.    ],\n",
       "       [169.    ,   0.7364,   1.    ],\n",
       "       [422.    ,   0.7367,   1.    ],\n",
       "       [430.    ,   0.7373,   1.    ],\n",
       "       [139.    ,   0.7404,   1.    ],\n",
       "       [121.    ,   0.7412,   1.    ],\n",
       "       [701.    ,   0.7421,   1.    ],\n",
       "       [574.    ,   0.7444,   1.    ],\n",
       "       [593.    ,   0.7474,   1.    ],\n",
       "       [630.    ,   0.7495,   1.    ],\n",
       "       [765.    ,   0.7507,   1.    ],\n",
       "       [816.    ,   0.7519,   1.    ],\n",
       "       [230.    ,   0.7528,   1.    ],\n",
       "       [112.    ,   0.7553,   1.    ],\n",
       "       [511.    ,   0.7554,   1.    ],\n",
       "       [780.    ,   0.7586,   1.    ],\n",
       "       [792.    ,   0.7614,   1.    ],\n",
       "       [526.    ,   0.7644,   1.    ],\n",
       "       [794.    ,   0.7662,   1.    ],\n",
       "       [566.    ,   0.7726,   1.    ],\n",
       "       [400.    ,   0.7744,   1.    ],\n",
       "       [152.    ,   0.7744,   1.    ],\n",
       "       [393.    ,   0.7776,   1.    ],\n",
       "       [435.    ,   0.7816,   1.    ],\n",
       "       [244.    ,   0.7817,   1.    ],\n",
       "       [776.    ,   0.7821,   1.    ],\n",
       "       [568.    ,   0.7834,   1.    ],\n",
       "       [  8.    ,   0.7849,   1.    ],\n",
       "       [895.    ,   0.7861,   1.    ],\n",
       "       [497.    ,   0.787 ,   1.    ],\n",
       "       [ 70.    ,   0.7908,   1.    ],\n",
       "       [681.    ,   0.7926,   1.    ],\n",
       "       [197.    ,   0.7937,   1.    ],\n",
       "       [141.    ,   0.794 ,   1.    ],\n",
       "       [473.    ,   0.796 ,   1.    ],\n",
       "       [490.    ,   0.7979,   1.    ],\n",
       "       [418.    ,   0.7991,   1.    ],\n",
       "       [455.    ,   0.7996,   1.    ],\n",
       "       [842.    ,   0.7999,   1.    ],\n",
       "       [734.    ,   0.8006,   1.    ],\n",
       "       [517.    ,   0.8016,   1.    ],\n",
       "       [363.    ,   0.8031,   1.    ],\n",
       "       [773.    ,   0.8036,   1.    ],\n",
       "       [ 71.    ,   0.8039,   1.    ],\n",
       "       [672.    ,   0.8043,   1.    ],\n",
       "       [636.    ,   0.8048,   1.    ],\n",
       "       [707.    ,   0.8101,   1.    ],\n",
       "       [897.    ,   0.8108,   1.    ],\n",
       "       [100.    ,   0.8136,   1.    ],\n",
       "       [664.    ,   0.8157,   1.    ],\n",
       "       [686.    ,   0.8216,   1.    ],\n",
       "       [530.    ,   0.8219,   1.    ],\n",
       "       [355.    ,   0.8244,   1.    ],\n",
       "       [806.    ,   0.8249,   1.    ],\n",
       "       [609.    ,   0.8251,   1.    ],\n",
       "       [  0.    ,   0.8251,   1.    ],\n",
       "       [154.    ,   0.8253,   1.    ],\n",
       "       [348.    ,   0.8267,   1.    ],\n",
       "       [104.    ,   0.828 ,   1.    ],\n",
       "       [498.    ,   0.8287,   1.    ],\n",
       "       [673.    ,   0.8289,   1.    ],\n",
       "       [124.    ,   0.8293,   1.    ],\n",
       "       [352.    ,   0.831 ,   1.    ],\n",
       "       [551.    ,   0.8318,   1.    ],\n",
       "       [508.    ,   0.8319,   1.    ],\n",
       "       [250.    ,   0.8319,   1.    ],\n",
       "       [756.    ,   0.8332,   1.    ],\n",
       "       [761.    ,   0.8337,   1.    ],\n",
       "       [702.    ,   0.8339,   1.    ],\n",
       "       [535.    ,   0.8354,   1.    ],\n",
       "       [808.    ,   0.8371,   1.    ],\n",
       "       [162.    ,   0.8379,   1.    ],\n",
       "       [620.    ,   0.8413,   1.    ],\n",
       "       [561.    ,   0.8414,   1.    ],\n",
       "       [610.    ,   0.8418,   1.    ],\n",
       "       [651.    ,   0.8424,   1.    ],\n",
       "       [228.    ,   0.8431,   1.    ],\n",
       "       [420.    ,   0.8437,   1.    ],\n",
       "       [117.    ,   0.8438,   1.    ],\n",
       "       [757.    ,   0.8445,   1.    ],\n",
       "       [804.    ,   0.8454,   1.    ],\n",
       "       [814.    ,   0.846 ,   1.    ],\n",
       "       [529.    ,   0.8488,   1.    ],\n",
       "       [434.    ,   0.8495,   1.    ],\n",
       "       [243.    ,   0.8523,   1.    ],\n",
       "       [424.    ,   0.8544,   1.    ],\n",
       "       [545.    ,   0.856 ,   1.    ],\n",
       "       [594.    ,   0.8568,   1.    ],\n",
       "       [180.    ,   0.8579,   1.    ],\n",
       "       [759.    ,   0.859 ,   1.    ],\n",
       "       [892.    ,   0.861 ,   1.    ],\n",
       "       [890.    ,   0.8642,   1.    ],\n",
       "       [287.    ,   0.8646,   1.    ],\n",
       "       [330.    ,   0.8655,   1.    ],\n",
       "       [221.    ,   0.8679,   1.    ],\n",
       "       [721.    ,   0.8682,   1.    ],\n",
       "       [625.    ,   0.8695,   1.    ],\n",
       "       [372.    ,   0.8702,   1.    ],\n",
       "       [388.    ,   0.8712,   1.    ],\n",
       "       [409.    ,   0.8726,   1.    ],\n",
       "       [187.    ,   0.873 ,   1.    ],\n",
       "       [357.    ,   0.8758,   1.    ],\n",
       "       [633.    ,   0.8758,   1.    ],\n",
       "       [320.    ,   0.8758,   1.    ],\n",
       "       [ 12.    ,   0.879 ,   1.    ],\n",
       "       [798.    ,   0.8792,   1.    ],\n",
       "       [506.    ,   0.8794,   1.    ],\n",
       "       [656.    ,   0.8801,   1.    ],\n",
       "       [569.    ,   0.8812,   1.    ],\n",
       "       [283.    ,   0.8818,   1.    ],\n",
       "       [373.    ,   0.8824,   1.    ],\n",
       "       [396.    ,   0.885 ,   1.    ],\n",
       "       [700.    ,   0.8879,   1.    ],\n",
       "       [472.    ,   0.8913,   1.    ],\n",
       "       [ 85.    ,   0.8928,   1.    ],\n",
       "       [752.    ,   0.8933,   1.    ],\n",
       "       [299.    ,   0.8946,   1.    ],\n",
       "       [813.    ,   0.8955,   1.    ],\n",
       "       [ 93.    ,   0.9044,   1.    ],\n",
       "       [489.    ,   0.905 ,   1.    ],\n",
       "       [224.    ,   0.9063,   1.    ],\n",
       "       [851.    ,   0.9075,   1.    ],\n",
       "       [286.    ,   0.9118,   1.    ],\n",
       "       [266.    ,   0.9136,   1.    ],\n",
       "       [191.    ,   0.9156,   1.    ],\n",
       "       [401.    ,   0.9168,   1.    ],\n",
       "       [ 19.    ,   0.9191,   1.    ],\n",
       "       [873.    ,   0.9212,   1.    ],\n",
       "       [738.    ,   0.9225,   1.    ],\n",
       "       [755.    ,   0.9228,   1.    ],\n",
       "       [109.    ,   0.9241,   1.    ],\n",
       "       [202.    ,   0.9253,   1.    ],\n",
       "       [267.    ,   0.9259,   1.    ],\n",
       "       [ 31.    ,   0.926 ,   1.    ],\n",
       "       [115.    ,   0.9265,   1.    ],\n",
       "       [391.    ,   0.9267,   1.    ],\n",
       "       [500.    ,   0.9313,   1.    ],\n",
       "       [168.    ,   0.932 ,   1.    ],\n",
       "       [709.    ,   0.9352,   1.    ],\n",
       "       [ 79.    ,   0.9353,   1.    ],\n",
       "       [531.    ,   0.9355,   1.    ],\n",
       "       [309.    ,   0.9357,   1.    ],\n",
       "       [ 68.    ,   0.937 ,   1.    ],\n",
       "       [ 78.    ,   0.9371,   1.    ],\n",
       "       [748.    ,   0.9376,   1.    ],\n",
       "       [323.    ,   0.9378,   1.    ],\n",
       "       [322.    ,   0.938 ,   1.    ],\n",
       "       [ 23.    ,   0.9381,   1.    ],\n",
       "       [547.    ,   0.939 ,   1.    ],\n",
       "       [548.    ,   0.94  ,   1.    ],\n",
       "       [173.    ,   0.9416,   1.    ],\n",
       "       [590.    ,   0.9418,   1.    ],\n",
       "       [ 33.    ,   0.9428,   1.    ],\n",
       "       [304.    ,   0.9441,   1.    ],\n",
       "       [864.    ,   0.9462,   1.    ],\n",
       "       [140.    ,   0.9469,   1.    ],\n",
       "       [237.    ,   0.9481,   1.    ],\n",
       "       [ 91.    ,   0.9491,   1.    ],\n",
       "       [240.    ,   0.9516,   1.    ],\n",
       "       [693.    ,   0.9525,   1.    ],\n",
       "       [209.    ,   0.953 ,   1.    ],\n",
       "       [333.    ,   0.9556,   1.    ],\n",
       "       [621.    ,   0.9557,   1.    ],\n",
       "       [750.    ,   0.957 ,   1.    ],\n",
       "       [722.    ,   0.9577,   1.    ],\n",
       "       [789.    ,   0.9581,   1.    ],\n",
       "       [254.    ,   0.9589,   1.    ],\n",
       "       [274.    ,   0.96  ,   1.    ],\n",
       "       [730.    ,   0.9626,   1.    ],\n",
       "       [470.    ,   0.9631,   1.    ],\n",
       "       [461.    ,   0.9638,   1.    ],\n",
       "       [126.    ,   0.9649,   1.    ],\n",
       "       [468.    ,   0.9684,   1.    ],\n",
       "       [ 49.    ,   0.9701,   1.    ],\n",
       "       [128.    ,   0.9707,   1.    ],\n",
       "       [110.    ,   0.9712,   1.    ],\n",
       "       [658.    ,   0.9724,   1.    ],\n",
       "       [839.    ,   0.9736,   1.    ],\n",
       "       [826.    ,   0.9755,   1.    ],\n",
       "       [247.    ,   0.976 ,   1.    ],\n",
       "       [354.    ,   0.9763,   1.    ],\n",
       "       [532.    ,   0.977 ,   1.    ],\n",
       "       [157.    ,   0.9783,   1.    ],\n",
       "       [745.    ,   0.9829,   1.    ],\n",
       "       [605.    ,   0.9831,   1.    ],\n",
       "       [261.    ,   0.9837,   1.    ],\n",
       "       [875.    ,   0.984 ,   1.    ],\n",
       "       [  6.    ,   0.9843,   1.    ],\n",
       "       [273.    ,   0.9853,   1.    ],\n",
       "       [406.    ,   0.9872,   1.    ],\n",
       "       [571.    ,   0.9875,   1.    ],\n",
       "       [670.    ,   0.9881,   1.    ],\n",
       "       [344.    ,   0.9904,   1.    ],\n",
       "       [810.    ,   0.9906,   1.    ],\n",
       "       [744.    ,   0.9909,   1.    ],\n",
       "       [118.    ,   0.9911,   1.    ],\n",
       "       [615.    ,   0.9949,   1.    ],\n",
       "       [725.    ,   0.9957,   1.    ],\n",
       "       [ 28.    ,   0.9971,   1.    ],\n",
       "       [436.    ,   0.9971,   1.    ],\n",
       "       [ 60.    ,   0.9985,   1.    ],\n",
       "       [294.    ,   0.999 ,   1.    ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the prediction values\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c0665",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6502e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df= pd.read_csv(\"face_data.csv\", usecols=[\"Image Path\",\"Face\",\"Bounding Box\",\"Target\"]) \n",
    "eval_df=eval_df[eval_df[\"Face\"]==1]\n",
    "# display(eval_df.head())\n",
    "\n",
    "results=[]\n",
    "for i in tqdm(eval_df.iterrows()):\n",
    "    img=cv2.imread(i[1][\"Image Path\"])\n",
    "    img=cv2.resize(img, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
    "    bb=eval(i[1][\"Bounding Box\"])\n",
    "    crp_img=img[bb[1]:bb[3],bb[0]:bb[2]]\n",
    "    result,__=fr.detect(crp_img, 0.5)\n",
    "    if result:results.append(int(list(result.keys())[0]))\n",
    "    else:results.append(None)  \n",
    "eval_df[\"Predicted\"]=results \n",
    "display(eval_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d6bd914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Confusion Matrix===================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_true = eval_df[\"Target\"].tolist()\n",
    "y_pred = eval_df[\"Predicted\"].tolist()\n",
    "print(\"============== Confusion Matrix===================\")\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4507c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative:0, True positive: 1, False negative:1, False positive:2\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "print(f\"True negative:{tn}, True positive: {tp}, False negative:{fn}, False positive:{fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01be82c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++Classification Report++++++++++++++++++++++++\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00        13\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.00      0.00      0.00         9\n",
      "           8       0.00      0.00      0.00         6\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         9\n",
      "          11       0.00      0.00      0.00         7\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00         9\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         5\n",
      "          17       0.00      0.00      0.00         7\n",
      "          18       0.00      0.00      0.00         7\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         6\n",
      "          21       0.00      0.00      0.00         5\n",
      "          22       0.00      0.00      0.00         3\n",
      "          23       0.00      0.00      0.00         7\n",
      "          24       0.00      0.00      0.00         5\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00         3\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        12\n",
      "          29       0.00      0.00      0.00        21\n",
      "          30       0.00      0.00      0.00         4\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.00      0.00      0.00         3\n",
      "          33       0.00      0.00      0.00        10\n",
      "          34       0.00      0.00      0.00         4\n",
      "          35       0.00      0.00      0.00         3\n",
      "          36       0.00      0.00      0.00        14\n",
      "          37       0.00      0.00      0.00        10\n",
      "          38       0.00      0.00      0.00         4\n",
      "          39       0.00      0.00      0.00        10\n",
      "          40       0.00      0.00      0.00         6\n",
      "          41       0.00      0.00      0.00        10\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         3\n",
      "          44       0.00      0.00      0.00         4\n",
      "          45       0.00      0.00      0.00        14\n",
      "          46       0.00      0.00      0.00         3\n",
      "          47       0.00      0.00      0.00         4\n",
      "          48       0.00      0.00      0.00         3\n",
      "          49       0.00      0.00      0.00         3\n",
      "          50       0.00      0.00      0.00         5\n",
      "          51       0.00      0.00      0.00         4\n",
      "          52       0.00      0.00      0.00         4\n",
      "          53       0.00      0.00      0.00        11\n",
      "          54       0.00      0.00      0.00         4\n",
      "          55       0.00      0.00      0.00         5\n",
      "          56       0.00      0.00      0.00         3\n",
      "          57       0.00      0.00      0.00         3\n",
      "          58       0.00      0.00      0.00         5\n",
      "          59       0.00      0.00      0.00         3\n",
      "          60       0.00      0.00      0.00        17\n",
      "          61       0.00      0.00      0.00         8\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         9\n",
      "          64       0.00      0.00      0.00         4\n",
      "          65       0.00      0.00      0.00         3\n",
      "          66       0.00      0.00      0.00         6\n",
      "          67       0.00      0.00      0.00        18\n",
      "          68       0.00      0.00      0.00         3\n",
      "          69       0.00      0.00      0.00         5\n",
      "          70       0.00      0.00      0.00        10\n",
      "          71       0.00      0.00      0.00        17\n",
      "          72       0.00      0.00      0.00         3\n",
      "          73       0.00      0.00      0.00         8\n",
      "          74       0.00      0.00      0.00         3\n",
      "          75       0.00      0.00      0.00         4\n",
      "          76       0.00      0.00      0.00         5\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.00      0.00      0.00        25\n",
      "          79       0.00      0.00      0.00         9\n",
      "          80       0.00      0.00      0.00         5\n",
      "          81       0.00      0.00      0.00        35\n",
      "          82       0.00      0.00      0.00        10\n",
      "          83       0.00      0.00      0.00        20\n",
      "          84       0.00      0.00      0.00         3\n",
      "          85       0.00      0.00      0.00         7\n",
      "          86       0.00      0.00      0.00         5\n",
      "          87       0.00      0.00      0.00         3\n",
      "          88       0.00      0.00      0.00         3\n",
      "          89       0.00      0.00      0.00        17\n",
      "          90       0.00      0.00      0.00         7\n",
      "          91       0.00      0.00      0.00         4\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       0.00      0.00      0.00         4\n",
      "          94       0.00      0.00      0.00         3\n",
      "          95       0.00      0.00      0.00         8\n",
      "          96       0.00      0.00      0.00        10\n",
      "          97       0.00      0.00      0.00        11\n",
      "          98       0.00      0.00      0.00         3\n",
      "          99       0.00      0.00      0.00         3\n",
      "         100       0.00      0.00      0.00         3\n",
      "         101       0.00      0.00      0.00         3\n",
      "         102       0.00      0.00      0.00         3\n",
      "         103       0.00      0.00      0.00         6\n",
      "         104       0.00      0.00      0.00         3\n",
      "         105       0.00      0.00      0.00         5\n",
      "         106       0.00      0.00      0.00         4\n",
      "         107       0.00      0.00      0.00         3\n",
      "         108       0.00      0.00      0.00         2\n",
      "         109       0.00      0.00      0.00         3\n",
      "         110       0.00      0.00      0.00         3\n",
      "         111       0.00      0.00      0.00        15\n",
      "         112       0.00      0.00      0.00         3\n",
      "         113       0.00      0.00      0.00         4\n",
      "         114       0.00      0.00      0.00         3\n",
      "         115       0.00      0.00      0.00         7\n",
      "         116       0.00      0.00      0.00         4\n",
      "         117       0.00      0.00      0.00         3\n",
      "         118       0.00      0.00      0.00         3\n",
      "         119       0.00      0.00      0.00         3\n",
      "         120       0.00      0.00      0.00         5\n",
      "         121       0.00      0.00      0.00         3\n",
      "         122       0.00      0.00      0.00         3\n",
      "         123       0.00      0.00      0.00         4\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         3\n",
      "         126       0.00      0.00      0.00         5\n",
      "         127       0.00      0.00      0.00         9\n",
      "         128       0.00      0.00      0.00        18\n",
      "         129       0.00      0.00      0.00         6\n",
      "         130       0.00      0.00      0.00         4\n",
      "         131       0.00      0.00      0.00         4\n",
      "         132       0.00      0.00      0.00        14\n",
      "         133       0.00      0.00      0.00         3\n",
      "         134       0.00      0.00      0.00        16\n",
      "         135       0.00      0.00      0.00         5\n",
      "         136       0.00      0.00      0.00         4\n",
      "         137       0.00      0.00      0.00         4\n",
      "         138       0.00      0.00      0.00         3\n",
      "         139       0.00      0.00      0.00         3\n",
      "         140       0.00      0.00      0.00         5\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       0.00      0.00      0.00         9\n",
      "         143       0.00      0.00      0.00         7\n",
      "         144       0.00      0.00      0.00         3\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.00      0.00      0.00         3\n",
      "         147       0.00      0.00      0.00        27\n",
      "         148       0.00      0.00      0.00         3\n",
      "         149       0.00      0.00      0.00        10\n",
      "         150       0.00      0.00      0.00         7\n",
      "         151       0.00      0.00      0.00        20\n",
      "         152       0.00      0.00      0.00         5\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.00      0.00      0.00         3\n",
      "         155       0.00      0.00      0.00         7\n",
      "         156       0.00      0.00      0.00         4\n",
      "         157       0.00      0.00      0.00         3\n",
      "         158       0.00      0.00      0.00         3\n",
      "         159       0.00      0.00      0.00        13\n",
      "         160       0.00      0.00      0.00        20\n",
      "         161       0.00      0.00      0.00         5\n",
      "         162       0.00      0.00      0.00         5\n",
      "         163       0.00      0.00      0.00        10\n",
      "         164       0.00      0.00      0.00         4\n",
      "         165       0.00      0.00      0.00         3\n",
      "         166       0.00      0.00      0.00         5\n",
      "         167       0.00      0.00      0.00         7\n",
      "         168       0.00      0.00      0.00         4\n",
      "         169       0.00      0.00      0.00         4\n",
      "         170       0.00      0.00      0.00        31\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       0.00      0.00      0.00        12\n",
      "         173       0.00      0.00      0.00         6\n",
      "         174       0.00      0.00      0.00         4\n",
      "         175       0.00      0.00      0.00         4\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         3\n",
      "         178       0.00      0.00      0.00         6\n",
      "         179       0.00      0.00      0.00         5\n",
      "         180       0.00      0.00      0.00         4\n",
      "         181       0.00      0.00      0.00         4\n",
      "         182       0.00      0.00      0.00         3\n",
      "         183       0.00      0.00      0.00         4\n",
      "         184       0.00      0.00      0.00         8\n",
      "         185       0.00      0.00      0.00         5\n",
      "         186       0.00      0.00      0.00         4\n",
      "         187       0.00      0.00      0.00         3\n",
      "         188       0.00      0.00      0.00         4\n",
      "         189       0.00      0.00      0.00         4\n",
      "         190       0.00      0.00      0.00        10\n",
      "         191       0.00      0.00      0.00        14\n",
      "         192       0.00      0.00      0.00         5\n",
      "         193       0.00      0.00      0.00        19\n",
      "         194       0.00      0.00      0.00         4\n",
      "         195       0.00      0.00      0.00         5\n",
      "         196       0.00      0.00      0.00         6\n",
      "         197       0.00      0.00      0.00         3\n",
      "         198       0.00      0.00      0.00         4\n",
      "         199       0.00      0.00      0.00         5\n",
      "         200       0.00      0.00      0.00        50\n",
      "         201       0.00      0.00      0.00         3\n",
      "         202       0.00      0.00      0.00         3\n",
      "         203       0.00      0.00      0.00        15\n",
      "         204       0.00      0.00      0.00         7\n",
      "         205       0.00      0.00      0.00         7\n",
      "         206       0.00      0.00      0.00         3\n",
      "         207       0.00      0.00      0.00         3\n",
      "         208       0.00      0.00      0.00         3\n",
      "         209       0.00      0.00      0.00         7\n",
      "         210       0.00      0.00      0.00         4\n",
      "         211       0.00      0.00      0.00         3\n",
      "         212       0.00      0.00      0.00        25\n",
      "         213       0.00      0.00      0.00         3\n",
      "         214       0.00      0.00      0.00       109\n",
      "         215       0.00      0.00      0.00         7\n",
      "         216       0.00      0.00      0.00         5\n",
      "         217       0.00      0.00      0.00         3\n",
      "         218       0.00      0.00      0.00         4\n",
      "         219       0.00      0.00      0.00         7\n",
      "         220       0.00      0.00      0.00         6\n",
      "         221       0.00      0.00      0.00         8\n",
      "         222       0.00      0.00      0.00        23\n",
      "         223       0.00      0.00      0.00        32\n",
      "         224       0.00      0.00      0.00         3\n",
      "         225       0.00      0.00      0.00         9\n",
      "         226       0.00      0.00      0.00         3\n",
      "         227       0.00      0.00      0.00        21\n",
      "         228       0.00      0.00      0.00         3\n",
      "         229       0.00      0.00      0.00         3\n",
      "         230       0.00      0.00      0.00         4\n",
      "         231       0.00      0.00      0.00         6\n",
      "         232       0.00      0.00      0.00        49\n",
      "         233       0.00      0.00      0.00        21\n",
      "         234       0.00      0.00      0.00        15\n",
      "         235       0.00      0.00      0.00         3\n",
      "         236       0.00      0.00      0.00         3\n",
      "         237       0.00      0.00      0.00         3\n",
      "         238       0.00      0.00      0.00         3\n",
      "         239       0.00      0.00      0.00       144\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         5\n",
      "         242       0.00      0.00      0.00         4\n",
      "         243       0.00      0.00      0.00         4\n",
      "         244       0.00      0.00      0.00         4\n",
      "         245       0.00      0.00      0.00        11\n",
      "         246       0.00      0.00      0.00         6\n",
      "         247       0.00      0.00      0.00         3\n",
      "         248       0.00      0.00      0.00         5\n",
      "         249       0.00      0.00      0.00         4\n",
      "         250       0.00      0.00      0.00         3\n",
      "         251       0.00      0.00      0.00         6\n",
      "         252       0.00      0.00      0.00        10\n",
      "         253       0.00      0.00      0.00         6\n",
      "         254       0.00      0.00      0.00         5\n",
      "         255       0.00      0.00      0.00        28\n",
      "         256       0.00      0.00      0.00        22\n",
      "         257       0.00      0.00      0.00        11\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         6\n",
      "         260       0.00      0.00      0.00         3\n",
      "         261       0.00      0.00      0.00         5\n",
      "         262       0.00      0.00      0.00         6\n",
      "         263       0.00      0.00      0.00        20\n",
      "         264       0.00      0.00      0.00        29\n",
      "         265       0.00      0.00      0.00         4\n",
      "         266       0.00      0.00      0.00         3\n",
      "         267       0.00      0.00      0.00         3\n",
      "         268       0.00      0.00      0.00         4\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       0.00      0.00      0.00         3\n",
      "         271       0.00      0.00      0.00         8\n",
      "         272       0.00      0.00      0.00         4\n",
      "         273       0.00      0.00      0.00         3\n",
      "         274       0.00      0.00      0.00         3\n",
      "         275       0.00      0.00      0.00        18\n",
      "         276       0.00      0.00      0.00         3\n",
      "         277       0.00      0.00      0.00         5\n",
      "         278       0.00      0.00      0.00         4\n",
      "         279       0.00      0.00      0.00        12\n",
      "         280       0.00      0.00      0.00         4\n",
      "         281       0.00      0.00      0.00         4\n",
      "         282       0.00      0.00      0.00        37\n",
      "         283       0.00      0.00      0.00         4\n",
      "         284       0.00      0.00      0.00         3\n",
      "         285       0.00      0.00      0.00         3\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         6\n",
      "         288       0.00      0.00      0.00         3\n",
      "         289       0.00      0.00      0.00         4\n",
      "         290       0.00      0.00      0.00         6\n",
      "         291       0.00      0.00      0.00         3\n",
      "         292       0.00      0.00      0.00         3\n",
      "         293       0.00      0.00      0.00         7\n",
      "         294       0.00      1.00      0.00         3\n",
      "         295       0.00      0.00      0.00         4\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.00      0.00      0.00         3\n",
      "         298       0.00      0.00      0.00         3\n",
      "         299       0.00      0.00      0.00         3\n",
      "         300       0.00      0.00      0.00         3\n",
      "         301       0.00      0.00      0.00         8\n",
      "         302       0.00      0.00      0.00         5\n",
      "         303       0.00      0.00      0.00         3\n",
      "         304       0.00      0.00      0.00         3\n",
      "         305       0.00      0.00      0.00        10\n",
      "         306       0.00      0.00      0.00         5\n",
      "         307       0.00      0.00      0.00        22\n",
      "         308       0.00      0.00      0.00         3\n",
      "         309       0.00      0.00      0.00         3\n",
      "         310       0.00      0.00      0.00         4\n",
      "         311       0.00      0.00      0.00         4\n",
      "         312       0.00      0.00      0.00        18\n",
      "         313       0.00      0.00      0.00        12\n",
      "         314       0.00      0.00      0.00         6\n",
      "         315       0.00      0.00      0.00         4\n",
      "         316       0.00      0.00      0.00         5\n",
      "         317       0.00      0.00      0.00         4\n",
      "         318       0.00      0.00      0.00         3\n",
      "         319       0.00      0.00      0.00         3\n",
      "         320       0.00      0.00      0.00         3\n",
      "         321       0.00      0.00      0.00         4\n",
      "         322       0.00      0.00      0.00         5\n",
      "         323       0.00      0.00      0.00        17\n",
      "         324       0.00      0.00      0.00         6\n",
      "         325       0.00      0.00      0.00         5\n",
      "         326       0.00      0.00      0.00         3\n",
      "         327       0.00      0.00      0.00         4\n",
      "         328       0.00      0.00      0.00        13\n",
      "         329       0.00      0.00      0.00         4\n",
      "         330       0.00      0.00      0.00         8\n",
      "         331       0.00      0.00      0.00         3\n",
      "         332       0.00      0.00      0.00         4\n",
      "         333       0.00      0.00      0.00         6\n",
      "         334       0.00      0.00      0.00         4\n",
      "         335       0.00      0.00      0.00         3\n",
      "         336       0.00      0.00      0.00         3\n",
      "         337       0.00      0.00      0.00         3\n",
      "         338       0.00      0.00      0.00        32\n",
      "         339       0.00      0.00      0.00        10\n",
      "         340       0.00      0.00      0.00         3\n",
      "         341       0.00      0.00      0.00         3\n",
      "         342       0.00      0.00      0.00        48\n",
      "         343       0.00      0.00      0.00         3\n",
      "         344       0.00      0.00      0.00         5\n",
      "         345       0.00      0.00      0.00         9\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         4\n",
      "         348       0.00      0.00      0.00        14\n",
      "         349       0.00      0.00      0.00         6\n",
      "         350       0.00      0.00      0.00         4\n",
      "         351       0.00      0.00      0.00         3\n",
      "         352       0.00      0.00      0.00         3\n",
      "         353       0.00      0.00      0.00         3\n",
      "         354       0.00      0.00      0.00        20\n",
      "         355       0.00      0.00      0.00         4\n",
      "         356       0.00      0.00      0.00         9\n",
      "         357       0.00      0.00      0.00         3\n",
      "         358       0.00      0.00      0.00         4\n",
      "         359       0.00      0.00      0.00         5\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       0.00      0.00      0.00         3\n",
      "         362       0.00      0.00      0.00         4\n",
      "         363       0.00      0.00      0.00         3\n",
      "         364       0.00      0.00      0.00         6\n",
      "         365       0.00      0.00      0.00         5\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       0.00      0.00      0.00         6\n",
      "         368       0.00      0.00      0.00         4\n",
      "         369       0.00      0.00      0.00        21\n",
      "         370       0.00      0.00      0.00         6\n",
      "         371       0.00      0.00      0.00        11\n",
      "         372       0.00      0.00      0.00         3\n",
      "         373       0.00      0.00      0.00         3\n",
      "         374       0.00      0.00      0.00         3\n",
      "         375       0.00      0.00      0.00         5\n",
      "         376       0.00      0.00      0.00         4\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         3\n",
      "         379       0.00      0.00      0.00         3\n",
      "         380       0.00      0.00      0.00         3\n",
      "         381       0.00      0.00      0.00         3\n",
      "         382       0.00      0.00      0.00         5\n",
      "         383       0.00      0.00      0.00         4\n",
      "         384       0.00      0.00      0.00         3\n",
      "         385       0.00      0.00      0.00         3\n",
      "         386       0.00      0.00      0.00         3\n",
      "         387       0.00      0.00      0.00        11\n",
      "         388       0.00      0.00      0.00         3\n",
      "         389       0.00      0.00      0.00         5\n",
      "         390       0.00      0.00      0.00        10\n",
      "         391       0.00      0.00      0.00         3\n",
      "         392       0.00      0.00      0.00         3\n",
      "         393       0.00      0.00      0.00         4\n",
      "         394       0.00      0.00      0.00         8\n",
      "         395       0.00      0.00      0.00         4\n",
      "         396       0.00      0.00      0.00         6\n",
      "         397       0.00      0.00      0.00         3\n",
      "         398       0.00      0.00      0.00         5\n",
      "         399       0.00      0.00      0.00         4\n",
      "         400       0.00      0.00      0.00         5\n",
      "         401       0.00      0.00      0.00         3\n",
      "         402       0.00      0.00      0.00         5\n",
      "         403       0.00      0.00      0.00         3\n",
      "         404       0.00      0.00      0.00         3\n",
      "         405       0.00      0.00      0.00         6\n",
      "         406       0.00      0.00      0.00        14\n",
      "         407       0.00      0.00      0.00         4\n",
      "         408       0.00      0.00      0.00         3\n",
      "         409       0.00      0.00      0.00         5\n",
      "         410       0.00      0.00      0.00         8\n",
      "         411       0.00      0.00      0.00         7\n",
      "         412       0.00      0.00      0.00         6\n",
      "         413       0.00      0.00      0.00        13\n",
      "         414       0.00      0.00      0.00         3\n",
      "         415       0.00      0.00      0.00         5\n",
      "         416       0.00      0.00      0.00         3\n",
      "         417       0.00      0.00      0.00         3\n",
      "         418       0.00      0.00      0.00         5\n",
      "         419       0.00      0.00      0.00         4\n",
      "         420       0.00      0.00      0.00         4\n",
      "         421       0.00      0.00      0.00         3\n",
      "         422       0.00      0.00      0.00         3\n",
      "         423       0.00      0.00      0.00         4\n",
      "         424       0.00      0.00      0.00         4\n",
      "         425       0.00      0.00      0.00         3\n",
      "         426       0.00      0.00      0.00         6\n",
      "         427       0.00      0.00      0.00         3\n",
      "         428       0.00      0.00      0.00         3\n",
      "         429       0.00      0.00      0.00        24\n",
      "         430       0.00      0.00      0.00        11\n",
      "         431       0.00      0.00      0.00         5\n",
      "         432       0.00      0.00      0.00         5\n",
      "         433       0.00      0.00      0.00        33\n",
      "         434       0.00      0.00      0.00         7\n",
      "         435       0.00      0.00      0.00         3\n",
      "         436       0.00      0.00      0.00         3\n",
      "         437       0.00      0.00      0.00         4\n",
      "         438       0.00      0.00      0.00         4\n",
      "         439       0.00      0.00      0.00         4\n",
      "         440       0.00      0.00      0.00        55\n",
      "         441       0.00      0.00      0.00         3\n",
      "         442       0.00      0.00      0.00         5\n",
      "         443       0.00      0.00      0.00         3\n",
      "         444       0.00      0.00      0.00         4\n",
      "         445       0.00      0.00      0.00         5\n",
      "         446       0.00      0.00      0.00         8\n",
      "         447       0.00      0.00      0.00         3\n",
      "         448       0.00      0.00      0.00         6\n",
      "         449       0.00      0.00      0.00        15\n",
      "         450       0.00      0.00      0.00         2\n",
      "         451       0.00      0.00      0.00         3\n",
      "         452       0.00      0.00      0.00         3\n",
      "         453       0.00      0.00      0.00        36\n",
      "         454       0.00      0.00      0.00         8\n",
      "         455       0.00      0.00      0.00         6\n",
      "         456       0.00      0.00      0.00         3\n",
      "         457       0.00      0.00      0.00         4\n",
      "         458       0.00      0.00      0.00         3\n",
      "         459       0.00      0.00      0.00         4\n",
      "         460       0.00      0.00      0.00        17\n",
      "         461       0.00      0.00      0.00        31\n",
      "         462       0.00      0.00      0.00       120\n",
      "         463       0.00      0.00      0.00         3\n",
      "         464       0.00      0.00      0.00         3\n",
      "         465       0.00      0.00      0.00         3\n",
      "         466       0.00      0.00      0.00         4\n",
      "         467       0.00      0.00      0.00        41\n",
      "         468       0.00      0.00      0.00        41\n",
      "         469       0.00      0.00      0.00         8\n",
      "         470       0.00      0.00      0.00        10\n",
      "         471       0.00      0.00      0.00         9\n",
      "         472       0.00      0.00      0.00        13\n",
      "         473       0.00      0.00      0.00         4\n",
      "         474       0.00      0.00      0.00         7\n",
      "         475       0.00      0.00      0.00         4\n",
      "         476       0.00      0.00      0.00         3\n",
      "         477       0.00      0.00      0.00         4\n",
      "         478       0.00      0.00      0.00         4\n",
      "         479       0.00      0.00      0.00         7\n",
      "         480       0.00      0.00      0.00         3\n",
      "         481       0.00      0.00      0.00         8\n",
      "         482       0.00      0.00      0.00         6\n",
      "         483       0.00      0.00      0.00         3\n",
      "         484       0.00      0.00      0.00         6\n",
      "         485       0.00      0.00      0.00         4\n",
      "         486       0.00      0.00      0.00         3\n",
      "         487       0.00      0.00      0.00         5\n",
      "         488       0.00      0.00      0.00         3\n",
      "         489       0.00      0.00      0.00         4\n",
      "         490       0.00      0.00      0.00         3\n",
      "         491       0.00      0.00      0.00         3\n",
      "         492       0.00      0.00      0.00         3\n",
      "         493       0.00      0.00      0.00        28\n",
      "         494       0.00      0.00      0.00         3\n",
      "         495       0.00      0.00      0.00         4\n",
      "         496       0.00      0.00      0.00         4\n",
      "         497       0.00      0.00      0.00         4\n",
      "         498       0.00      0.00      0.00         3\n",
      "         499       0.00      0.00      0.00         4\n",
      "         500       0.00      0.00      0.00         3\n",
      "         501       0.00      0.00      0.00         3\n",
      "         502       0.00      0.00      0.00        12\n",
      "         503       0.00      0.00      0.00         5\n",
      "         504       0.00      0.00      0.00         3\n",
      "         505       0.00      0.00      0.00         4\n",
      "         506       0.00      0.00      0.00         9\n",
      "         507       0.00      0.00      0.00         4\n",
      "         508       0.00      0.00      0.00         6\n",
      "         509       0.00      0.00      0.00         4\n",
      "         510       0.00      0.00      0.00         5\n",
      "         511       0.00      0.00      0.00         4\n",
      "         512       0.00      0.00      0.00         9\n",
      "         513       0.00      0.00      0.00         4\n",
      "         514       0.00      0.00      0.00         5\n",
      "         515       0.00      0.00      0.00         4\n",
      "         516       0.00      0.00      0.00         5\n",
      "         517       0.00      0.00      0.00         5\n",
      "         518       0.00      0.00      0.00         3\n",
      "         519       0.00      0.00      0.00         3\n",
      "         520       0.00      0.00      0.00         4\n",
      "         521       0.00      0.00      0.00         4\n",
      "         522       0.00      0.00      0.00         4\n",
      "         523       0.00      0.00      0.00         8\n",
      "         524       0.00      0.00      0.00         3\n",
      "         525       0.00      0.00      0.00         9\n",
      "         526       0.00      0.00      0.00         6\n",
      "         527       0.00      0.00      0.00         6\n",
      "         528       0.00      0.00      0.00         8\n",
      "         529       0.00      0.00      0.00        10\n",
      "         530       0.00      0.00      0.00         3\n",
      "         531       0.00      0.00      0.00         4\n",
      "         532       0.00      0.00      0.00         5\n",
      "         533       0.00      0.00      0.00         5\n",
      "         534       0.00      0.00      0.00         6\n",
      "         535       0.00      0.00      0.00         6\n",
      "         536       0.00      0.00      0.00         3\n",
      "         537       0.00      0.00      0.00         4\n",
      "         538       0.00      0.00      0.00         3\n",
      "         539       0.00      0.00      0.00         4\n",
      "         540       0.00      0.00      0.00         5\n",
      "         541       0.00      0.00      0.00         8\n",
      "         542       0.00      0.00      0.00         4\n",
      "         543       0.00      0.00      0.00         4\n",
      "         544       0.00      0.00      0.00       235\n",
      "         545       0.00      0.00      0.00        11\n",
      "         546       0.00      0.00      0.00         3\n",
      "         547       0.00      0.00      0.00         6\n",
      "         548       0.00      0.00      0.00         4\n",
      "         549       0.00      0.00      0.00         8\n",
      "         550       0.00      0.00      0.00         9\n",
      "         551       0.00      0.00      0.00         3\n",
      "         552       0.00      0.00      0.00         3\n",
      "         553       0.00      0.00      0.00         3\n",
      "         554       0.00      0.00      0.00        14\n",
      "         555       0.00      0.00      0.00         3\n",
      "         556       0.00      0.00      0.00         9\n",
      "         557       0.00      0.00      0.00         4\n",
      "         558       0.00      0.00      0.00        22\n",
      "         559       0.00      0.00      0.00         8\n",
      "         560       0.00      0.00      0.00        12\n",
      "         561       0.00      0.00      0.00         3\n",
      "         562       0.00      0.00      0.00        11\n",
      "         563       0.00      0.00      0.00         5\n",
      "         564       0.00      0.00      0.00         3\n",
      "         565       0.00      0.00      0.00         5\n",
      "         566       0.00      0.00      0.00        27\n",
      "         567       0.00      0.00      0.00         3\n",
      "         568       0.00      0.00      0.00         3\n",
      "         569       0.00      0.00      0.00        13\n",
      "         570       0.00      0.00      0.00         8\n",
      "         571       0.00      0.00      0.00         2\n",
      "         572       0.00      0.00      0.00         4\n",
      "         573       0.00      0.00      0.00         7\n",
      "         574       0.00      0.00      0.00        15\n",
      "         575       0.00      0.00      0.00         3\n",
      "         576       0.00      0.00      0.00         4\n",
      "         577       0.00      0.00      0.00         7\n",
      "         578       0.00      0.00      0.00         4\n",
      "         579       0.00      0.00      0.00         5\n",
      "         580       0.00      0.00      0.00         3\n",
      "         581       0.00      0.00      0.00         3\n",
      "         582       0.00      0.00      0.00         8\n",
      "         583       0.00      0.00      0.00         3\n",
      "         584       0.00      0.00      0.00         3\n",
      "         585       0.00      0.00      0.00         8\n",
      "         586       0.00      0.00      0.00        24\n",
      "         587       0.00      0.00      0.00         4\n",
      "         588       0.00      0.00      0.00         3\n",
      "         589       0.00      0.00      0.00         5\n",
      "         590       0.00      0.00      0.00         8\n",
      "         591       0.00      0.00      0.00         7\n",
      "         592       0.00      0.00      0.00         3\n",
      "         593       0.00      0.00      0.00         4\n",
      "         594       0.00      0.00      0.00        11\n",
      "         595       0.00      0.00      0.00        12\n",
      "         596       0.00      0.00      0.00         3\n",
      "         597       0.00      0.00      0.00         5\n",
      "         598       0.00      0.00      0.00         8\n",
      "         599       0.00      0.00      0.00        11\n",
      "         600       0.00      0.00      0.00         4\n",
      "         601       0.00      0.00      0.00         4\n",
      "         602       0.00      0.00      0.00         4\n",
      "         603       0.00      0.00      0.00         5\n",
      "         604       0.00      0.00      0.00         3\n",
      "         605       0.00      0.00      0.00        17\n",
      "         606       0.00      0.00      0.00         3\n",
      "         607       0.00      0.00      0.00         5\n",
      "         608       0.00      0.00      0.00         3\n",
      "         609       0.00      0.00      0.00         4\n",
      "         610       0.00      0.00      0.00         4\n",
      "         611       0.00      0.00      0.00         4\n",
      "         612       0.00      0.00      0.00         9\n",
      "         613       0.00      0.00      0.00         3\n",
      "         614       0.00      0.00      0.00        22\n",
      "         615       0.00      0.00      0.00         3\n",
      "         616       0.00      0.00      0.00         3\n",
      "         617       0.00      0.00      0.00         3\n",
      "         618       0.00      0.00      0.00         3\n",
      "         619       0.00      0.00      0.00         5\n",
      "         620       0.00      0.00      0.00         3\n",
      "         621       0.00      0.00      0.00         3\n",
      "         622       0.00      0.00      0.00         3\n",
      "         623       0.00      0.00      0.00         3\n",
      "         624       0.00      0.00      0.00         4\n",
      "         625       0.00      0.00      0.00         4\n",
      "         626       0.00      0.00      0.00         5\n",
      "         627       0.00      0.00      0.00         3\n",
      "         628       0.00      0.00      0.00         3\n",
      "         629       0.00      0.00      0.00         3\n",
      "         630       0.00      0.00      0.00         3\n",
      "         631       0.00      0.00      0.00        13\n",
      "         632       0.00      0.00      0.00         4\n",
      "         633       0.00      0.00      0.00         3\n",
      "         634       0.00      0.00      0.00         3\n",
      "         635       0.00      0.00      0.00         3\n",
      "         636       0.00      0.00      0.00        11\n",
      "         637       0.00      0.00      0.00         3\n",
      "         638       0.00      0.00      0.00        11\n",
      "         639       0.00      0.00      0.00         5\n",
      "         640       0.00      0.00      0.00         3\n",
      "         641       0.00      0.00      0.00         6\n",
      "         642       0.00      0.00      0.00         3\n",
      "         643       0.00      0.00      0.00        14\n",
      "         644       0.00      0.00      0.00         5\n",
      "         645       0.00      0.00      0.00         3\n",
      "         646       0.00      0.00      0.00         9\n",
      "         647       0.00      0.00      0.00         3\n",
      "         648       0.00      0.00      0.00         7\n",
      "         649       0.00      0.00      0.00         3\n",
      "         650       0.00      0.00      0.00         5\n",
      "         651       0.00      0.00      0.00         8\n",
      "         652       0.00      0.00      0.00         4\n",
      "         653       0.00      0.00      0.00         8\n",
      "         654       0.00      0.00      0.00        17\n",
      "         655       0.00      0.00      0.00         4\n",
      "         656       0.00      0.00      0.00         4\n",
      "         657       0.00      0.00      0.00         3\n",
      "         658       0.00      0.00      0.00        71\n",
      "         659       0.00      0.00      0.00        19\n",
      "         660       0.00      0.00      0.00         4\n",
      "         661       0.00      0.00      0.00        19\n",
      "         662       0.00      0.00      0.00         5\n",
      "         663       0.00      0.00      0.00         3\n",
      "         664       0.00      0.00      0.00         5\n",
      "         665       0.00      0.00      0.00        12\n",
      "         666       0.00      0.00      0.00         3\n",
      "         667       0.00      0.00      0.00         7\n",
      "         668       0.00      0.00      0.00         4\n",
      "         669       0.00      0.00      0.00        42\n",
      "         670       0.00      0.00      0.00         7\n",
      "         671       0.00      0.00      0.00         5\n",
      "         672       0.00      0.00      0.00         3\n",
      "         673       0.00      0.00      0.00         3\n",
      "         674       0.00      0.00      0.00         3\n",
      "         675       0.00      0.00      0.00        16\n",
      "         676       0.00      0.00      0.00         3\n",
      "         677       0.00      0.00      0.00         3\n",
      "         678       0.00      0.00      0.00        10\n",
      "         679       0.00      0.00      0.00         4\n",
      "         680       0.00      0.00      0.00         3\n",
      "         681       0.00      0.00      0.00         3\n",
      "         682       0.00      0.00      0.00         3\n",
      "         683       0.00      0.00      0.00         4\n",
      "         684       0.00      0.00      0.00         4\n",
      "         685       0.00      0.00      0.00         3\n",
      "         686       0.00      0.00      0.00        19\n",
      "         687       0.00      0.00      0.00         9\n",
      "         688       0.00      0.00      0.00        23\n",
      "         689       0.00      0.00      0.00         7\n",
      "         690       0.00      0.00      0.00         4\n",
      "         691       0.00      0.00      0.00         5\n",
      "         692       0.00      0.00      0.00         7\n",
      "         693       0.00      0.00      0.00         4\n",
      "         694       0.00      0.00      0.00         4\n",
      "         695       0.00      0.00      0.00         4\n",
      "         696       0.00      0.00      0.00         8\n",
      "         697       0.00      0.00      0.00         3\n",
      "         698       0.00      0.00      0.00         5\n",
      "         699       0.00      0.00      0.00         8\n",
      "         700       0.00      0.00      0.00         6\n",
      "         701       0.00      0.00      0.00         8\n",
      "         702       0.00      0.00      0.00         3\n",
      "         703       0.00      0.00      0.00         8\n",
      "         704       0.00      0.00      0.00         3\n",
      "         705       0.00      0.00      0.00         6\n",
      "         706       0.00      0.00      0.00         4\n",
      "         707       0.00      0.00      0.00         5\n",
      "         708       0.00      0.00      0.00        19\n",
      "         709       0.00      0.00      0.00         4\n",
      "         710       0.00      0.00      0.00         3\n",
      "         711       0.00      0.00      0.00        14\n",
      "         712       0.00      0.00      0.00         5\n",
      "         713       0.00      0.00      0.00         3\n",
      "         714       0.00      0.00      0.00         3\n",
      "         715       0.00      0.00      0.00         8\n",
      "         716       0.00      0.00      0.00         6\n",
      "         717       0.00      0.00      0.00         4\n",
      "         718       0.00      0.00      0.00         3\n",
      "         719       0.00      0.00      0.00         4\n",
      "         720       0.00      0.00      0.00        60\n",
      "         721       0.00      0.00      0.00         5\n",
      "         722       0.00      0.00      0.00         3\n",
      "         723       0.00      0.00      0.00         4\n",
      "         724       0.00      0.00      0.00         3\n",
      "         725       0.00      0.00      0.00         3\n",
      "         726       0.00      0.00      0.00         3\n",
      "         727       0.00      0.00      0.00         4\n",
      "         728       0.00      0.00      0.00         5\n",
      "         729       0.00      0.00      0.00         3\n",
      "         730       0.00      0.00      0.00         4\n",
      "         731       0.00      0.00      0.00         9\n",
      "         732       0.00      0.00      0.00         7\n",
      "         733       0.00      0.00      0.00        44\n",
      "         734       0.00      0.00      0.00         5\n",
      "         735       0.00      0.00      0.00         5\n",
      "         736       0.00      0.00      0.00         7\n",
      "         737       0.00      0.00      0.00         7\n",
      "         738       0.00      0.00      0.00         3\n",
      "         739       0.00      0.00      0.00         3\n",
      "         740       0.00      0.00      0.00        17\n",
      "         741       0.00      0.00      0.00         3\n",
      "         742       0.00      0.00      0.00         5\n",
      "         743       0.00      0.00      0.00         3\n",
      "         744       0.00      0.00      0.00         5\n",
      "         745       0.00      0.00      0.00         3\n",
      "         746       0.00      0.00      0.00        13\n",
      "         747       0.00      0.00      0.00        39\n",
      "         748       0.00      0.00      0.00         7\n",
      "         749       0.00      0.00      0.00         4\n",
      "         750       0.00      0.00      0.00         3\n",
      "         751       0.00      0.00      0.00         4\n",
      "         752       0.00      0.00      0.00         9\n",
      "         753       0.00      0.00      0.00        39\n",
      "         754       0.00      0.00      0.00         5\n",
      "         755       0.00      0.00      0.00         9\n",
      "         756       0.00      0.00      0.00        19\n",
      "         757       0.00      0.00      0.00         3\n",
      "         758       0.00      0.00      0.00        75\n",
      "         759       0.00      0.00      0.00         3\n",
      "         760       0.00      0.00      0.00        30\n",
      "         761       0.00      0.00      0.00         6\n",
      "         762       0.00      0.00      0.00         3\n",
      "         763       0.00      0.00      0.00         3\n",
      "         764       0.00      0.00      0.00        14\n",
      "         765       0.00      0.00      0.00        22\n",
      "         766       0.00      0.00      0.00         5\n",
      "         767       0.00      0.00      0.00         4\n",
      "         768       0.00      0.00      0.00        15\n",
      "         769       0.00      0.00      0.00         4\n",
      "         770       0.00      0.00      0.00         5\n",
      "         771       0.00      0.00      0.00         3\n",
      "         772       0.00      0.00      0.00         6\n",
      "         773       0.00      0.00      0.00         3\n",
      "         774       0.00      0.00      0.00         5\n",
      "         775       0.00      0.00      0.00         3\n",
      "         776       0.00      0.00      0.00        32\n",
      "         777       0.00      0.00      0.00         5\n",
      "         778       0.00      0.00      0.00         3\n",
      "         779       0.00      0.00      0.00        13\n",
      "         780       0.00      0.00      0.00       528\n",
      "         781       0.00      0.00      0.00         6\n",
      "         782       0.00      0.00      0.00         9\n",
      "         783       0.00      0.00      0.00         3\n",
      "         784       0.00      0.00      0.00         3\n",
      "         785       0.00      0.00      0.00        33\n",
      "         786       0.00      0.00      0.00         6\n",
      "         787       0.00      0.00      0.00        15\n",
      "         788       0.00      0.00      0.00        20\n",
      "         789       0.00      0.00      0.00         3\n",
      "         790       0.00      0.00      0.00         3\n",
      "         791       0.00      0.00      0.00         5\n",
      "         792       0.00      0.00      0.00         4\n",
      "         793       0.00      0.00      0.00        12\n",
      "         794       0.00      0.00      0.00         9\n",
      "         795       0.00      0.00      0.00         3\n",
      "         796       0.00      0.00      0.00        24\n",
      "         797       0.00      0.00      0.00         9\n",
      "         798       0.00      0.00      0.00         4\n",
      "         799       0.00      0.00      0.00         4\n",
      "         800       0.00      0.00      0.00         6\n",
      "         801       0.00      0.00      0.00        11\n",
      "         802       0.00      0.00      0.00         3\n",
      "         803       0.00      0.00      0.00         5\n",
      "         804       0.00      0.00      0.00        52\n",
      "         805       0.00      0.00      0.00         3\n",
      "         806       0.00      0.00      0.00         3\n",
      "         807       0.00      0.00      0.00        14\n",
      "         808       0.00      0.00      0.00         3\n",
      "         809       0.00      0.00      0.00         7\n",
      "         810       0.00      0.00      0.00         4\n",
      "         811       0.00      0.00      0.00         5\n",
      "         812       0.00      0.00      0.00         3\n",
      "         813       0.00      0.00      0.00         7\n",
      "         814       0.00      0.00      0.00         6\n",
      "         815       0.00      0.00      0.00         3\n",
      "         816       0.00      0.00      0.00         4\n",
      "         817       0.00      0.00      0.00         4\n",
      "         818       0.00      0.00      0.00         3\n",
      "         819       0.00      0.00      0.00         4\n",
      "         820       0.00      0.00      0.00         4\n",
      "         821       0.00      0.00      0.00        33\n",
      "         822       0.00      0.00      0.00         5\n",
      "         823       0.00      0.00      0.00         4\n",
      "         824       0.00      0.00      0.00         3\n",
      "         825       0.00      0.00      0.00         3\n",
      "         826       0.00      0.00      0.00        17\n",
      "         827       0.00      0.00      0.00         4\n",
      "         828       0.00      0.00      0.00         6\n",
      "         829       0.00      0.00      0.00         4\n",
      "         830       0.00      0.00      0.00         2\n",
      "         831       0.00      0.00      0.00         3\n",
      "         832       0.00      0.00      0.00         4\n",
      "         833       0.00      0.00      0.00         6\n",
      "         834       0.00      0.00      0.00         4\n",
      "         835       0.00      0.00      0.00         3\n",
      "         836       0.00      0.00      0.00         3\n",
      "         837       0.00      0.00      0.00         4\n",
      "         838       0.00      0.00      0.00         3\n",
      "         839       0.00      0.00      0.00         5\n",
      "         840       0.00      0.00      0.00         5\n",
      "         841       0.00      0.00      0.00         4\n",
      "         842       0.00      0.00      0.00        25\n",
      "         843       0.00      0.00      0.00         3\n",
      "         844       0.00      0.00      0.00         5\n",
      "         845       0.00      0.00      0.00         5\n",
      "         846       0.00      0.00      0.00         3\n",
      "         847       0.00      0.00      0.00         5\n",
      "         848       0.00      0.00      0.00         4\n",
      "         849       0.00      0.00      0.00         3\n",
      "         850       0.00      0.00      0.00         4\n",
      "         851       0.00      0.00      0.00         6\n",
      "         852       0.00      0.00      0.00        13\n",
      "         853       0.00      0.00      0.00        30\n",
      "         854       0.00      0.00      0.00         5\n",
      "         855       0.00      0.00      0.00         3\n",
      "         856       0.00      0.00      0.00         4\n",
      "         857       0.00      0.00      0.00         5\n",
      "         858       0.00      0.00      0.00         6\n",
      "         859       0.00      0.00      0.00         4\n",
      "         860       0.00      0.00      0.00         5\n",
      "         861       0.00      0.00      0.00         5\n",
      "         862       0.00      0.00      0.00         3\n",
      "         863       0.00      0.00      0.00         4\n",
      "         864       0.00      0.00      0.00         4\n",
      "         865       0.00      0.00      0.00        14\n",
      "         866       0.00      0.00      0.00         3\n",
      "         867       0.00      0.00      0.00         3\n",
      "         868       0.00      0.00      0.00         4\n",
      "         869       0.00      0.00      0.00         3\n",
      "         870       0.00      0.00      0.00        18\n",
      "         871       0.00      0.00      0.00         9\n",
      "         872       0.00      0.00      0.00         4\n",
      "         873       0.00      0.00      0.00         3\n",
      "         874       0.00      0.00      0.00        11\n",
      "         875       0.00      0.00      0.00         4\n",
      "         876       0.00      0.00      0.00         3\n",
      "         877       0.00      0.00      0.00         4\n",
      "         878       0.00      0.00      0.00         3\n",
      "         879       0.00      0.00      0.00         5\n",
      "         880       0.00      0.00      0.00        16\n",
      "         881       0.00      0.00      0.00         4\n",
      "         882       0.00      0.00      0.00         3\n",
      "         883       0.00      0.00      0.00         6\n",
      "         884       0.00      0.00      0.00         6\n",
      "         885       0.00      0.00      0.00        41\n",
      "         886       0.00      0.00      0.00         8\n",
      "         887       0.00      0.00      0.00         7\n",
      "         888       0.00      0.00      0.00        53\n",
      "         889       0.00      0.00      0.00         4\n",
      "         890       0.00      0.00      0.00         5\n",
      "         891       0.00      0.00      0.00         3\n",
      "         892       0.00      0.00      0.00        15\n",
      "         893       0.00      0.00      0.00         3\n",
      "         894       0.00      0.00      0.00         3\n",
      "         895       0.00      0.00      0.00         3\n",
      "         896       0.00      0.00      0.00        14\n",
      "         897       0.00      0.00      0.00         7\n",
      "         898       0.00      0.00      0.00         4\n",
      "         899       0.00      0.00      0.00         5\n",
      "         900       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.00      7557\n",
      "   macro avg       0.00      0.00      0.00      7557\n",
      "weighted avg       0.00      0.00      0.00      7557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rakes\\anaconda3\\envs\\productTemplate\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"+++++++++++Classification Report++++++++++++++++++++++++\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "producttemplate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
