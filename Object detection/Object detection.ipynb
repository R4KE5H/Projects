{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "    \n",
    "Step 1: Read the image using opencv\n",
    "    \n",
    "Step 2: Pass the image to Yolo Network for detection\n",
    "    \n",
    "Step 3: Trained Objects are detected [x,y,w,h] format     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure:\n",
    "    \n",
    "1. Dataset is annotated using \"LabelImg\" tool in \"YOLO\" format in python [reference: https://pypi.org/project/labelImg/1.4.0/]\n",
    "\n",
    "2. \"data\" folder is created to training. [It contains \"obj\" folder, \"obj.names\", \"obj.data\", \"train.txt\", \"val.txt\" and \"yolov3-tiny-obj.cfg\" files]\n",
    "\n",
    "3. In \"yolov3-tiny-obj.cfg\", We need to change the no of classes that we have annotated and also some hyper parameter tuning regarding our custom dataset.\n",
    "\n",
    "4. After preparing \"data\" folder training is done using through darknet.[Reference:\"Yolo_custom_Training.ipynb\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate \"train.txt or val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \".\\data\\obj\"\n",
    "img_name = []\n",
    "for filename in (os.listdir(path)):\n",
    "    Image_file = filename #dumy file name\n",
    "    file = filename.split('.') #code to split the filename to check it is json r not\n",
    "    if file[1] == 'png' or file[1] == 'jpeg' or file[1] == 'jpg':\n",
    "        rename = file[0] + \".jpg\"\n",
    "        os.rename(path + '/' + filename, path + '/' + rename)\n",
    "        if os.path.exists(path + '/' + file[0] + '.txt') and os.path.exists(path + '/' + file[0] + '.jpg'):\n",
    "            txt_out = (\"data/obj/\" + Image_file)\n",
    "            f = open(r\"./data/train.txt\", \"a+\")\n",
    "            f.write(txt_out)\n",
    "            f.write(\"\\n\")\n",
    "            f.close()\n",
    "print(\"\\n!!! Train.TXT Generated !!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependcies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(classes):\n",
    "    '''Used to load classes in list format'''\n",
    "    with open(classes, 'r') as f:\n",
    "        class_names = f.read().splitlines()\n",
    "    return class_names\n",
    "\n",
    "def get_output_format(box):\n",
    "    x, y, w, h = box\n",
    "    return int(x), int(y), int(x + w), int(y + h)\n",
    "\n",
    "def bounding_box(img, conf=0.2, nms_thresh=0.3):\n",
    "    '''This function detects the bounding box of given image'''\n",
    "    global net, classes, outputlayers\n",
    "    \n",
    "    result = {k: [] for k in classes}\n",
    "    confidences = {k: [] for k in classes}\n",
    "    boxes = {k: [] for k in classes}\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(outputlayers)\n",
    "    Height, Width, _ = img.shape\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf:\n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - (w / 2)\n",
    "                y = center_y - (h / 2)\n",
    "                confidences[classes[class_id]].append(float(confidence))\n",
    "                boxes[classes[class_id]].append([int(i) for i in [x, y, w, h]])\n",
    "    indices = {}\n",
    "    for class_name, box in boxes.items():\n",
    "        indices[class_name] = cv2.dnn.NMSBoxes(box, confidences[class_name], conf,nms_thresh)\n",
    "    \n",
    "    for key, index in indices.items():\n",
    "        for i in index:\n",
    "            try:\n",
    "                select = i[0]\n",
    "            except:  \n",
    "                select = i\n",
    "            x, y, w, h = boxes[key][select]\n",
    "#             x1,y1,x2,y2=int(x), int(y), int(x + w), int(y + h)\n",
    "            result[key].append((x, y, w, h))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_format_pd(res_dic,img_name,classes):\n",
    "    data=[]\n",
    "    for cls, det in res_dic.items():\n",
    "        for box in det:\n",
    "            data.append([img_name,cls,classes.index(cls),list(box)])\n",
    "    return data    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg_path=\"./Yolo/yolo_v3.cfg\"\n",
    "model_weights_path=\"./Yolo/yolov3-tiny.weights\"\n",
    "classes_txt_path=\"./Yolo/classes.txt\"\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(model_cfg_path, model_weights_path)\n",
    "classes = load_classes(classes_txt_path)\n",
    "layer_names = net.getLayerNames()\n",
    "try:\n",
    "    outputlayers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except:\n",
    "    outputlayers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "data=[]    \n",
    "for each_img in glob.glob('./Object_detection_TestSet/*.jpg'):\n",
    "    file=each_img.split('\\\\')[-1]\n",
    "    img=cv2.imread(each_img)\n",
    "    res=bounding_box(img)\n",
    "    data=data+data_format_pd(res,file,classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file_name              class  unique_id            coordinates\n",
      "0  Img10.jpg  Standard Vehicles          0   [1129, 264, 99, 121]\n",
      "1  Img10.jpg  Standard Vehicles          0     [753, 208, 55, 73]\n",
      "2  Img10.jpg  Standard Vehicles          0  [1459, 343, 123, 134]\n",
      "3  Img10.jpg       Motorcyclist          2     [864, 222, 33, 79]\n",
      "4  Img10.jpg         Pedestrian          3    [381, 120, 46, 242]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(data,columns=['file_name','class','unique_id','coordinates'])\n",
    "print(df.head())\n",
    "\n",
    "df.to_excel(\"sample.xlsx\") #dataframe to excel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
